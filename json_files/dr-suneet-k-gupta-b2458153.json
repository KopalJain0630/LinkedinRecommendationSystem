{
    "id": "dr-suneet-k-gupta-b2458153",
    "name": "Dr. Suneet K. Gupta",
    "city": "Noida, Uttar Pradesh, India",
    "country_code": "IN",
    "position": "Top 2% Scientist (Stanford) | Strategic Leader | Innovator in AI & International Relations | IQAC |\nDriving excellence in Research & Development, crafting impactful AI Curricula, and transforming ideas into action.",
    "posts": null,
    "groups": null,
    "current_company": {
        "link": "",
        "name": "Bennett University",
        "title": "Top 2% Scientist (Stanford) | Strategic Leader | Innovator in AI & International Relations | IQAC |\nDriving excellence in Research & Development, crafting impactful AI Curricula, and transforming ideas into action."
    },
    "experience": [
        {
            "title": "Bennett University",
            "location": "Gautam Buddha Nagar, Uttar Pradesh, India",
            "description_html": null,
            "duration": "6 years 9 months",
            "positions": [
                {
                    "subtitle": "Bennett University",
                    "meta": "Jul 2021 - Feb 2024 2 years 8 months",
                    "duration": "Jul 2021 Feb 2024 2 years 8 months",
                    "start_date": "Jul 2021",
                    "end_date": "Feb 2024",
                    "duration_short": "2 years 8 months",
                    "title": "Associate Professor",
                    "description_html": null
                },
                {
                    "subtitle": "BENNETT UNIVERSITY",
                    "meta": "Jun 2017 - Aug 2021 4 years 3 months",
                    "duration": "Jun 2017 Aug 2021 4 years 3 months",
                    "start_date": "Jun 2017",
                    "end_date": "Aug 2021",
                    "duration_short": "4 years 3 months",
                    "title": "ASSISTANT PROFESSOR",
                    "description_html": null
                }
            ],
            "company": "Bennett University",
            "url": "",
            "company_logo_url": null
        },
        {
            "title": "Asst. Professor",
            "description_html": "  <!---->",
            "duration": "Jan 2005 Aug 2021 16 years 8 months",
            "start_date": "Jan 2005",
            "end_date": "Aug 2021",
            "duration_short": "16 years 8 months",
            "company": "Academic",
            "company_logo_url": ""
        },
        {
            "title": "Director-Operation",
            "location": "Delhi, India",
            "description": "The key responsibilities:Strategic planning and execution:- Developing and implementing operational plans and procedures that align with objectives of leadingindia.ai.- Identifying and mitigating risks associated with the operations.- Overseeing the development and deployment of new technologies and processes.Resource management:- Managing leadingindia.ai budget and resources, including human resources, technology, and facilities.- Ensuring that leadingindia.ai has the right people, skills, and tools in place to achieve its goals.Performance management:- Monitoring and reporting on the performance of teams.- Identifying and implementing ways to improve operational efficiency.Data operations:-Analyzing data to identify trends and opportunities for improvement in operations.-Developing and implementing strategic plans for renewals, professional services.",
            "description_html": "The key responsibilities:Strategic planning and execution:- Developing and implementing operational plans and procedures that align with objectives of leadingindia.ai.- Identifying and mitigating risks associated with the operations.- Overseeing the development and deployment of new technologies and processes.Resource management:- Managing leadingindia.ai budget and resources, including human resources, technology, and facilities.- Ensuring that leadingindia.ai has the right people, skills, and tools in place to achieve its goals.Performance management:- Monitoring and reporting on the performance of teams.- Identifying and implementing ways to improve operational efficiency.Data operations:-Analyzing data to identify trends and opportunities for improvement in operations.-Developing and implementing strategic plans for renewals, professional services.",
            "duration": "Apr 2018 Feb 2020 1 year 11 months",
            "start_date": "Apr 2018",
            "end_date": "Feb 2020",
            "duration_short": "1 year 11 months",
            "company": "leadingindia.ai",
            "company_id": "leading-india-ai",
            "url": "",
            "company_logo_url": ""
        },
        {
            "title": "ABES Engineering College",
            "location": "Ghāziābād Area, India",
            "description_html": null,
            "duration": "11 months",
            "positions": [
                {
                    "subtitle": "ABES Engineering College",
                    "meta": "Dec 2016 - May 2017 6 months",
                    "duration": "Dec 2016 May 2017 6 months",
                    "start_date": "Dec 2016",
                    "end_date": "May 2017",
                    "duration_short": "6 months",
                    "title": "Associate Professor",
                    "description_html": null
                },
                {
                    "subtitle": "ABES Engineering College",
                    "meta": "Jul 2016 - Nov 2016 5 months",
                    "duration": "Jul 2016 Nov 2016 5 months",
                    "start_date": "Jul 2016",
                    "end_date": "Nov 2016",
                    "duration_short": "5 months",
                    "title": "Assistant Professor",
                    "description_html": null
                }
            ],
            "company": "ABES Engineering College",
            "url": "",
            "company_logo_url": null
        }
    ],
    "people_also_viewed": [
        {
            "profile_link": "",
            "name": "Dr.Swamynathan S M",
            "about": "Associate Professor-Dept.of ECE-Karpagam College of Engineering",
            "location": "Coimbatore"
        },
        {
            "profile_link": "",
            "name": "Christo Ananth",
            "about": "Professor",
            "location": "Tamil Nadu, India"
        },
        {
            "profile_link": "",
            "name": "Prof. (Dr.) Tapas Badal",
            "about": "Professor & Associate Dean (Academics and Evaluation) at Bennett University (Times of India Group)AI/DL, Image and video analytics, Game Design Development, AR/VR Application Development",
            "location": "New Delhi"
        },
        {
            "profile_link": "",
            "name": "Dr. Bineet Kumar Joshi",
            "about": "Associate Professor & Head, Innovation & Entrepreneurship Cell, ICFAI University, Dehradun,",
            "location": "Dehradun"
        },
        {
            "profile_link": "",
            "name": "Dr. Geraldine Amali",
            "about": "Associate Professor at Vellore Institute of Technology",
            "location": "Tamil Nadu, India"
        },
        {
            "profile_link": "",
            "name": "Dr Sudheer Hanumanthakari",
            "about": "Associate Professor at ICFAI Foundation for Higher Education, Hyderabad",
            "location": "Hyderabad"
        },
        {
            "profile_link": "",
            "name": "Ranjit Singh",
            "about": "Assistant Professor at RPS group of institutions",
            "location": "Kurukshetra"
        },
        {
            "profile_link": "",
            "name": "Dr. Shraddha Phansalkar",
            "about": "Professor Head (Computer Science Engg), MIT School of Computing MIT ADT University, Lead, Blockchain Centre of Excellence, Certified Blockchain Professional and Trainer",
            "location": "Pune"
        },
        {
            "profile_link": "",
            "name": "Dr. Abhishek Singh",
            "about": "T A At PWD Rajasthan",
            "location": "Rajasthan, India"
        },
        {
            "profile_link": "",
            "name": "Kavita Tewani",
            "about": "AI Scientist - Healthcare, Computational Drug Discovery",
            "location": "Bengaluru"
        }
    ],
    "educations_details": "Indian Institute of Technology (Indian School of Mines), Dhanbad",
    "education": [
        {
            "title": "Indian Institute of Technology (Indian School of Mines), Dhanbad",
            "degree": "PhD",
            "field": "Computer Science and Engineering",
            "url": "",
            "start_year": "2012",
            "end_year": "2016",
            "description": null,
            "description_html": null,
            "institute_logo_url": ""
        },
        {
            "title": "DIT DEHRADUN",
            "degree": "B.E. (CSE)",
            "field": "first class",
            "start_year": "1998",
            "end_year": "2002",
            "description": null,
            "description_html": null,
            "institute_logo_url": ""
        }
    ],
    "recommendations_count": 1.0,
    "courses": null,
    "languages": null,
    "certifications": [
        {
            "meta": "Issued Jan 2022Credential ID WF7ESC55R4LPSee credential",
            "subtitle": "Coursera",
            "title": "Blockchain Basics"
        },
        {
            "meta": "Issued Sep 2020Credential ID NUHGDK6X6F45See credential",
            "subtitle": "Coursera",
            "title": "Introduction to Big Data"
        },
        {
            "meta": "Issued Aug 2020Credential ID  credential",
            "subtitle": "Coursera",
            "title": "Algorithmica ToolBox"
        },
        {
            "meta": "Issued Aug 2020",
            "subtitle": "NVIDIA",
            "title": "Certificate for Appreciation"
        },
        {
            "meta": "Issued May 2020Credential ID CDPJDMNAERFPSee credential",
            "subtitle": "Coursera",
            "title": "Intro to TensorFlow"
        },
        {
            "meta": "Issued May 2020Credential ID AMKVJ8V2LWFGSee credential",
            "subtitle": "Coursera",
            "title": "Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning"
        },
        {
            "meta": "Issued May 2020Credential ID AL62SQTUK8L5See credential",
            "subtitle": "Coursera",
            "title": "Natural Language Processing in TensorFlow"
        },
        {
            "meta": "Issued Apr 2020See credential",
            "subtitle": "SAP",
            "title": "BW on HANA"
        },
        {
            "meta": "Issued Apr 2020See credential",
            "subtitle": "SAP",
            "title": "SAP Business Object Analysis "
        },
        {
            "meta": "Issued Mar 2020See credential",
            "subtitle": "Congnitive class",
            "title": "Block Chain Essential"
        },
        {
            "meta": "Issued Mar 2020Credential ID 78897490755dc143e4c9cc6939dcd732e742dc07See credential",
            "subtitle": "DataCamp",
            "title": "Certification on \"Introduction to R\""
        },
        {
            "meta": "Issued Mar 2020Credential ID ",
            "subtitle": "IBM",
            "title": "IBM Blockchain Essentials V2"
        },
        {
            "meta": "Issued Mar 2020See credential",
            "subtitle": "IBM",
            "title": "IBM Blockchain Essentials V2"
        },
        {
            "meta": "Issued Mar 2020See credential",
            "subtitle": "Cognitive Class",
            "title": "Python 101 for Data Science"
        },
        {
            "meta": "Issued Mar 2020See credential",
            "subtitle": "SoloLearn",
            "title": "Python 3"
        },
        {
            "meta": "Issued Mar 2020See credential",
            "subtitle": "freeCodeCamp",
            "title": "Responsive Web Design"
        },
        {
            "meta": "See credential",
            "subtitle": "Udemy",
            "title": "Blockchain and Bitcoin Fundamental "
        },
        {
            "meta": "See credential",
            "subtitle": "IBM",
            "title": "Python for Data Science"
        },
        {
            "meta": "See credential",
            "subtitle": "SAP SE",
            "title": "SAP Solution Knowledge - SAP Learning Hub User Onboarding"
        }
    ],
    "recommendations": [
        "Rajnish Kumar “Dr Suneet Gupta is a very nice person and fitness enthusiasts. He would always ready to accept challenges. Wish you all the very best for your future endeavors. ”"
    ],
    "volunteer_experience": null,
    "followers": 20051.0,
    "connections": 500.0,
    "current_company_company_id": null,
    "current_company_name": "Bennett University",
    "publications": "[{\"title\":\"Development of IoT enabled deep learning model for indian food classification: An approach based on differential evaluation\",\"subtitle\":\"Springer\",\"date\":\"November 1, 2024\",\"description\":\"Due to its extensive use in several areas, deep learning has attracted much interest in the past 10 years. Furthermore, decision-making applications for IoT devices are required, and the number of such devices is growing exponentially. Conversely, IoT devices are subject to resource constraints such as limited power, memory, and computation power. As a result, deep learning models that require less storage space and have a shorter inference time are more popular than traditional models. In the proposed article, we have discussed a differential evaluation-based approach for optimizing the storage space with a significant decrease in inference time without compromising the accuracy too much. We used an openly available Indian food dataset for the experimental work, using popular pre-trained architectures for classification purposes. We then compress the trained models using the differential evaluation approach. The simulation results show that the VGG16 architecture is compressed by 46.15%, with a decrease in precision of 1.91%.\"},{\"title\":\"The efficient classification of breast cancer on low-power iot devices: A study on genetically evolved u-net.\",\"subtitle\":\"Elsevier\",\"date\":\"November 1, 2024\",\"description\":\"Breast cancer is the most common cancer among women, and in some cases, it also affects men. Since early detection allows for proper treatment, automated data classification is essential. Although such classifications provide timely results, the resource requirements for such models, i.e., computation and storage, are high. As a result, these models are not suitable for resource-constrained devices (for example, IOT).In this work, we highlight the U-Net model, and to deploy it to IOT devices, we compress the same model using a genetic algorithm. We assess the proposed method using a publicly accessible, bench-marked dataset.To verify the efficacy of the suggested methodology, we conducted experiments on two more datasets, specifically CamVid and Potato leaf disease. In addition, we used the suggested method to shrink the MiniSegNet and FCN 32 models, which shows that the compressed U-Net approach works for classifying breast cancer. The results of the study indicate a significant decrease in the storage capacity of UNet with 96.12% compression for the breast cancer dataset with 1.97x enhancement in inference time. However, after compression of the model, there is a drop in accuracy of only 1.33%.\"},{\"title\":\"Blockchain, artificial intelligence, and healthcare: the tripod of future—a narrative review\",\"subtitle\":\"Springer\",\"date\":\"August 8, 2024\",\"description\":\"The fusion of blockchain and artificial intelligence (AI) marks a paradigm shift in healthcare, addressing critical challenges in securing electronic health records (EHRs), ensuring data privacy, and facilitating secure data transmission. This study provides a comprehensive analysis of the adoption of blockchain and AI within healthcare, spotlighting their role in fortifying security and transparency leading the trajectory for a promising future in the realm of healthcare. Our study, employing the PRISMA model, scrutinized 402 relevant articles, employing a narrative analysis to explore the fusion of blockchain and AI in healthcare. The review includes the architecture of AI and blockchain, examines AI applications with and without blockchain integration, and elucidates the interdependency between AI and blockchain. The major findings include: (i) it protects data transfer, and digital records, and provides security; (ii) enhances EHR security and COVID-19 data transmission, thereby bolstering healthcare efficiency and reliability through precise assessment metrics; (iii) addresses challenges like data security, privacy, and decentralized computing, forming a robust tripod. The fusion of blockchain and AI revolutionize healthcare by securing EHRs, and enhancing privacy, and security. Private blockchain adoption reflects the sector’s commitment to data security, leading to improved efficiency and accessibility. This convergence promises enhanced disease identification, response, and overall healthcare efficacy, and addresses key sector challenges. Further exploration of advanced AI features integrated with blockchain promises to enhance outcomes, shaping the future of global healthcare delivery with guaranteed data security, privacy, and innovation.\"},{\"title\":\"Enhancing sports image data classification in federated learning through genetic algorithm-based optimization of base architecture\",\"subtitle\":\"PLOS ONE\",\"date\":\"July 1, 2024\",\"description\":\"Nowadays, federated learning is one of the most prominent choices for making decisions. A significant benefit of federated learning is that, unlike deep learning, it is not necessary to share data samples with the model owner. The weight of the global model in traditional federated learning is created by averaging the weights of all clients or sites. In the proposed work, a novel method has been discussed to generate an optimized base model without hampering its performance, which is based on a genetic algorithm. Chromosome representation, crossover, and mutation-all the intermediate operations of the genetic algorithm have been illustrated with useful examples. After applying the genetic algorithm, there is a significant improvement in inference time and a huge reduction in storage space. Therefore, the model can be easily deployed on resource-constrained devices. For the experimental work, sports data has been used in balanced and unbalanced scenarios with various numbers of clients in a federated learning environment. In addition, we have used four famous deep learning architectures, such as AlexNet, VGG19, ResNet50, and EfficientNetB3, as the base model. We have achieved 92.34% accuracy with 9 clients in the balanced data set by using EfficientNetB3 as the base model using a GA-based approach. Moreover, after applying the genetic algorithm to optimize EfficientNetB3, there is an improvement in inference time and storage space by 20% and 2.35%, respectively.\"},{\"title\":\"nergy maximization for wireless powered communication enabled iot devices with noma underlaying solar powered uav using federated reinforcement learning for 6g networks.\",\"subtitle\":\"IEEE\",\"date\":\"January 22, 2024\",\"description\":\"The Internet of Things (IoT) depends primarily on low-cost wireless sensors with limited energy capacity to allow pervasive monitoring and intelligent control. Nevertheless, unmanned aerial vehicle (UAV) can be used to connect remote terminals that are outside wireless coverage to IoT networks. This solution provides a means of extending the reach of IoT networks, offering more opportunities for monitoring and control. Despite this benefit, the UAV also suffers from low capacity onboard battery. To overcome these problems, solar energy is integrated with UAV, and wireless-powered communication (WPC) techniques are used for IoT terminals. Also, the non-orthogonal multiple access (NOMA) technique can be employed to address the massive connectivity issue of IoT terminals. By leveraging these advantages, we jointly optimize the three-dimensional UAV trajectory and time allocation for WPC powered IoT devices (IoTDs) underlaying solar-powered UAV. To achieve the target, in this paper, introduces a multiagent federated reinforcement learning (MAFAL) algorithm, which concentrates on maximizing energy efficiency (EE) while minimizing energy consumption, guaranteeing quality of service (QoS), fairness, and trajectory planning. The proposed algorithm aims to optimize the overall performance of the system by learning from the collective experience of multiple agents. Simulation result demonstrated that the proposed method achieves 56.84%, 68.45%, and 73.63% higher EE as compared to MAD2PG, DDPG, and DQN, respectively.\"},{\"title\":\"ecacnn: differential evolution-based approach to compress and accelerate the convolution neural network model.\",\"subtitle\":\"Springer\",\"date\":\"November 1, 2023\",\"description\":\"In this research work, a differential evolution-based method has been used to compress the deep neural network architectures. The compression is achieved by selecting the most dominant filters/nodes during the training of the model. The usefulness of filters is established by the test accuracy of the model. The experimental results demonstrate that the performance of proposed model compares fairly well with other state of art model compression techniques. Moreover, the compression achieved with VGG16 on MNIST, CIFAR-10 and CIFAR-100 datasets was 98.32, 98.5 and 93.54%, respectively. The corresponding compression achieved on ResNet50 was 85.24, 85.38 and 79.37%, while SqueezeNet which is already compressed model could also be compressed by 72.94, 73.77 and 44.59%, respectively. MobileNet, which is already a compact model developed for mobile applications, could also be compressed by 93.04, 93.74 and 76.37% on MNIST, CIFAR-10 and CIFAR-100 datasets. The loss in accuracy in compressed models turns out to be less than 2%. Further, the compressed models report acceleration in inference time being 80.79% on VGG16, 74.14% on ResNet50, 42.96% on MobileNet and 11.79% on SqueezeNet.\"},{\"title\":\"Generalized framework using Federated Learning for tomato disease classification over unbalanced dataset\",\"subtitle\":\"ICCTA '23: Proceedings of the 2023 9th International Conference on Computer Technology Applications, ACM\",\"date\":\"August 20, 2023\",\"description\":\"Each cuisine required tomato in their kitchen for various food items and this makes tomato most popular crop worldwide and India is in second rank in terms production of tomato. Now a days, production of tomato goes down because of various diseases and to treat these diseases farmer needs to have extensive prior knowledge about the pathogen and along with various factor which promote the disease in the tomato. Due to lack of knowledge, the disease spreads rapidly and destroys all crops. To fill this gap, deep learning (DL) has been playing an important role, and there is much research on DL, how it can be used in medical industry and the agriculture industry for the identification of disease using images. There is a limitation for DL model that it does not work well with small dataset and huge amount of samples are required to train the model. Moreover, the data are not shared openly for security or for any other reason. Therefore, to overcome this challenge a Federated Learning (FL) based approach has been presented in the article. In FL, a deep learning model is shared with organizations which having the data and train the model. After training, the model information is shared with a centralized server which designs a generalized model. After getting the generalized model, it is shared with all other sites. The process is repeated until a generalized model is not designed and well-suited with all the sites. In our study, we tested our model on a tomato leaf disease data set using FL methodology with 10 clients and achieved the best precision with 88. 01%.\"},{\"title\":\"Artificial intelligence bias in medical system designs: a systematic review\",\"subtitle\":\"Multimedia Tools and Applications\",\"date\":\"July 22, 2023\",\"description\":\"Inherent bias in the artificial intelligence (AI)-model brings inaccuracies and variabilities during clinical deployment of the model. It is challenging to recognize the source of bias in AI-model due to variations in datasets and black box nature of system design. Additionally, there is no distinct process to identify the potential source of bias in the AI-model. To the best of our knowledge, this is the first review of its kind that addresses the bias in AI-model by categorizing 48 studies into three classes, namely, point-based, image-based, and hybrid-based AI-models. Selection strategy using PRISMA is adopted to select the 72 crucial AI studies for identifying bias in AI models. Using the three classes, bias is identified in these studies based on 44 critical AI attributes. Bias in the AI-models is computed by analytical, butterfly, and ranking-based bias models. These bias models were evaluated using two experts and compared using variability analysis. AI-studies that lacked sufficient AI-attributes are more prone to risk-of-bias (RoB) in all three classes. Studies with high RoB loses fins in the butterfly model. It has been analyzed that the majority of the studies in healthcare suffer from data bias and algorithmic bias due to incomplete specifications mentioned in the design protocol and weak AI design exploited for prediction.\"},{\"title\":\"Secure certificate sharing based on Blockchain framework for online education\",\"subtitle\":\"Multimedia Tools and Applications\",\"date\":\"May 1, 2023\",\"description\":\"In recent times, technology development has greatly influenced the model of Blockchain for communication due to extensive digitalization worldwide. Moreover, blockchain technology plays an essential part in the role of an educational system for sharing important data between students and teachers. However, the security hazard is the main problem while transferring the data because the malicious attacker may be able to hack the essential data in communication. A novel Signature-based Rivest Shamir Framework (SbRSF) is proposed to overcome such issues for privacy limit maintenance and security action improvement. Also, the harmful attack in data transmission is estimated by this projected approach. The implementation of this research has been done on the python platform. Hence, the proposed model has gained 98.03% throughput, encryption, and decryption time of 5 ms and reduced the error rate to 0.2%. Moreover, the achieved results are compared with the conventional methods, such as the Elliptic curve model, sensitivity-based elliptic crypto scheme, and secure record sharing, to validate the significance of the projected approach in blockchain development.\"},{\"title\":\"Development of a compressed FCN architecture for semantic segmentation using Particle Swarm Optimization\",\"subtitle\":\"Neural Computing and Application\",\"date\":\"March 2, 2023\",\"description\":\"Researchers have adapted the conventional deep learning classification networks to generate Fully Conventional Networks (FCN) for carrying out accurate semantic segmentation. However, such models are expensive both in terms of storage and inference time and not readily employable on edge devices. In this paper, a compressed version of VGG16-based Fully Convolution Network (FCN) has been developed using Particle Swarm Optimization. It has been shown that the developed model can offer tremendous saving in storage space and also faster inference time, and can be implemented on edge devices. The efficacy of the proposed approach has been tested using potato late blight leaf images from publicly available PlantVillage dataset, street scene image dataset and lungs X-Ray dataset and it has been shown that it approaches the accuracies offered by standard FCN even after 851× compression.\"},{\"title\":\"Genetic algorithm based approach to compress and accelerate the trained Convolution Neural Network model\",\"subtitle\":\"International Journal of Machine Learning and Cybernetics\",\"date\":\"January 16, 2023\",\"description\":\"Although transfer learning has been employed successfully with pre-trained models based on large convolutional neural networks, the demand for huge storage space makes it unattractive to deploy these solutions on edge devices having limited storage and computational power. A number of researchers have proposed Convolution Neural Network Compression models to take care of such issues. In this paper, a genetic algorithm-based approach has been employed to reduce the size of the Convolution Neural Network model, by selecting a subset of convolutional filters and nodes in the dense layers, while maintaining accuracy levels of original models. Specifically, AlexNet, VGG16, ResNet50 architectures have been taken up for model reduction and it has been shown that without compromising on the accuracy, huge gains can be made in terms of reduced storage space. The paper also shows that using this approach additional reduction in storage space of around 38% could be achieved even for SqueezeNet, which is an already compressed model. The paper also reports a substantial reduction in inference time for standard datasets such as MNIST, CIFAR-10 and CIFAR-100 applied on all the compressed models mentioned above. For CIFAR-100, the reduction in time is almost double that of other results reported in the literature.\"},{\"title\":\"UNet Deep Learning Architecture for Segmentation of Vascular and Non-Vascular Images: A Microscopic Look at UNet Components Buffered With Pruning, Explainable Artificial Intelligence, and Bias\",\"subtitle\":\"Ieee Access\",\"date\":\"December 26, 2022\",\"description\":\"Biomedical image segmentation (BIS) task is challenging due to the variations in organ types, position, shape, size, scale, orientation, and image contrast. Conventional methods lack accurate and automated designs. Artificial intelligence (AI)-based UNet has recently dominated BIS. This is the first review of its kind that microscopically addressed UNet types by complexity, stratification of UNet by its components, addressing UNet in vascular vs. non-vascular framework, the key to segmentation challenge vs. UNet-based architecture, and finally interfacing the three facets of AI, the pruning, the explainable AI (XAI), and the AI-bias. PRISMA was used to select 267 UNet-based studies. Five classes were identified and labeled as conventional UNet, superior UNet, attention-channel UNet, hybrid UNet, and ensemble UNet. We discovered 81 variations of UNet by considering six kinds of components, namely encoder, decoder, skip connection, bridge network, loss function, and their combination. Vascular vs. non-vascular UNet architecture was compared. AP(ai)Bias 2.0-UNet was identified in these UNet classes based on (i) attributes of UNet architecture and its performance, (ii) explainable AI (XAI), and, (iii) pruning (compression). Five bias methods such as (i) ranking, (ii) radial, (iii) regional area, (iv) PROBAST, and (v) ROBINS-I were applied and compared using a Venn diagram. Vascular and non-vascular UNet systems dominated with sUNet classes with attention. Most of the studies suffered from a low interest in XAI and pruning strategies. None of the UNet models qualified to be bias-free. There is a need to move from paper-to-practice paradigms for clinical evaluation and settings.\"},{\"title\":\"A novel genetic algorithm-based approach for compression and acceleration of deep learning convolution neural network: an application in computer tomography lung cancer data\",\"subtitle\":\"Neural Computing and Applications\",\"date\":\"December 1, 2022\",\"description\":\"Deep learning (DL) models are computationally expensive in space and time, which makes it difficult to deploy DL models in edge computing devices, such as Raspberry-Pi or Jetson Nano. The current strategy uses genetic algorithm (GA), which compresses the deep convolution neural network models without compromising performance. GA was applied by converting the CNN layers into binary vectors. Further, the fitness function in GA was computed based on (i) the minimization of hidden units and (ii) test accuracy. The GA-based strategy was applied on different pre-trained architectures, namely AlexNet, VGG16, SqueezeNet, and ResNet50, respectively, by using three kinds of datasets, namely MNIST, CIFAR-10, and CIFAR-100. The proposed approach demonstrated the reduction in the storage space of AlexNet by 87.62%, 80.97%, and 86.20% corresponding to the datasets MNIST, CIFAR-10, and CIFAR-100, respectively. Further, for the same three datasets, namely VGG16, ResNet50, and SqueezeNet, the system average compression was 91.15%, 78.42%, and 38.40%, respectively. In addition to that, the inference time of the models using proposed strategy was significantly improved with an average of the four datasets of ~ 35.61%, 9.23%, 73.76%, and 79.93% corresponding to AlexNet, SqueezeNet, ResNet50, and VGG16 models. Further, our method when applied to the proposed CNN using the LIDC-IDRI dataset showed a 90.3% reduction in the storage space and inference time. DL system when optimized using GA shows improved performance in both storage and execution time.\"},{\"title\":\"Artificial Intelligence and Blockchain Technologies for Smart City\",\"subtitle\":\"John Wiley & Sons, Inc.\",\"date\":\"November 10, 2022\",\"description\":\"In this digital age of rapid communication, the advances of emerging technologies can be used for making smart city as intelligent society. Specially, the speedy acceptance of artificial intelligence (AI) and blockchain technologies have guided a paradigm that is shifting to a new dimension called as digital ecosystem for smart city. A large number of AI and applications of blockchain guarantee resolutions for challenges in the fields varying from financial services, and threat management to cryptocurrency, and from social and public services to Internet of Things (IoT). Moreover, the conjunction of blockchain and AI technologies is transforming the network of smart city architecture for developing sustainable ecosystems. When we try to achieve the goal of developing smart cities, the innovations in technologies created both challenges and opportunities. This chapter presents a broad literature examination to the safety problems and the challenges, which influence the blockchain utilization in developing sustainable and smart societies. Our work represents a comprehensive dialogue of various vital issues for coming together for AI and blockchain knowledge, which help us to develop smart societies. Therefore, we talk about the solutions of security issues of blockchain and summarize the important concepts that need be utilized to develop many AI and blockchain-centered smart transportation techniques. Moreover, we review the problems that stay public and our forthcoming research directions, this contains new proposals for security and future regulations for developing a smart society with sustainable ecosystem.\"},{\"title\":\"Non-overlapping block-level difference-based image forgery detection and localization (NB-localization)\",\"subtitle\":\"The Visual Computer\",\"date\":\"November 2, 2022\",\"description\":\"With advent of digital devices, we are surrounded by many digital images. We usually believe on digital images in whatever form presented to us. Therefore, we need to be careful as the images may be forged. There exist several image forgeries through which original intent of the image may be hidden and some other meaning is reflected through forgery. Copy-move forgery is one such forgery technique, where the manipulator copies certain portion of the image and duplicates it in some other portion of the same image. In this paper, we propose a novel approach to detect the copy-move forgery in images using non-overlapping block level pixel comparisons and that can achieve better detection and classification accuracy. This approach divides image into 4, 5, 6 or more such blocks and compare each block by moving sliding window over the entire image which is not overlapping with current block. It was found that with different number of blocks the forged region of different sizes can be easily found. We have used SSIM (structure similarity index) parameter to classify the image as forged or original. Algorithm is simulated on various datasets including (MICC, CASIA, coverage, and COMOFOD, etc.) and achieved maximum accuracy of 98% and also compared our result on precision, recall, FPR and FNR including other parameters.\"},{\"title\":\"Deep learning artificial intelligence framework for multiclass coronary artery disease prediction using combination of conventional risk factors, carotid ultrasound, and intraplaque neovascularization\",\"subtitle\":\"Computers in Biology and Medicine\",\"date\":\"November 1, 2022\",\"description\":\"Cardiovascular disease (CVD) is a major healthcare challenge and therefore early risk assessment is vital. Previous assessment techniques use either “conventional CVD risk calculators (CCVRC)” or machine learning (ML) paradigms. These techniques are ad-hoc, unreliable, not fully automated, and have variabilities. We, therefore, introduce AtheroEdge- (AE3.0DL) windows-based platform using multiclass Deep Learning (DL) system.\"},{\"title\":\"VI-NET: A hybrid deep convolutional neural network using VGG and inception V3 model for copy-move forgery classification\",\"subtitle\":\"Journal of Visual Communication and Image Representation\",\"date\":\"November 1, 2022\",\"description\":\"Nowadays, various image editing tools are available that can be utilized for manipulating the original images; here copy-move forgery is most common forgery. In copy-move forgery, some part of the original image is copied and pasted into the same image at some other location. However, Artificial Intelligence (AI) based approaches can extract manipulated features easily. In this study, a deep learning-based method is proposed to classify the copy-move forged images. For classifying the forged images, a deep learning (DL) based hybrid model is presented named as VI-NET using fusion of two DL architectures, i.e., VGG16 and Inception V3. Further, output of two models is concatenated and connected with two additional convolutional layers. Cross-validation protocols, K10 (90 % training, 10 % testing), K5 (80 % training, 20 % testing), and K2 (50 % training, 50 % testing) are applied on the COMOFOD dataset. Moreover, the performance of VI-NET is compared with transfer learning and machine learning models using evaluation metrics such as accuracy, precision, recall, F1 score, etc. Proposed hybrid model performed better than other approaches with classification accuracy of 99 ± 0.2 % in comparison to accuracy of 95 ± 4 % (Inception V3), 93 ± 5 % (MobileNet), 59 ± 8 % (VGG16), 60 ± 1 % (Decision tree), 87 ± 1 % (KNN), 54 ± 1 % (Naïve Bayes) and 65 ± 1 % (random forest) under K10 protocol. Similarly, results are evaluated based on K2 and K5 validation protocols. It is experimentally observed that the proposed model performance is better than existing standard and customized deep learning architectures.\"},{\"title\":\"Optimized dual fire attention network and medium-scale fire classification benchmark\",\"subtitle\":\"IEEE Transactions on Image Processing\",\"date\":\"September 21, 2022\",\"description\":\"Vision-based fire detection systems have been significantly improved by deep models; however, higher numbers of false alarms and a slow inference speed still hinder their practical applicability in real-world scenarios. For a balanced trade-off between computational cost and accuracy, we introduce dual fire attention network (DFAN) to achieve effective yet efficient fire detection. The first attention mechanism highlights the most important channels from the features of an existing backbone model, yielding significantly emphasized feature maps. Then, a modified spatial attention mechanism is employed to capture spatial details and enhance the discrimination potential of fire and non-fire objects. We further optimize the DFAN for real-world applications by discarding a significant number of extra parameters using a meta-heuristic approach, which yields around 50% higher FPS values. Finally, we contribute a medium-scale challenging fire classification dataset by considering extremely diverse, highly similar fire/non-fire images and imbalanced classes, among many other complexities. The proposed dataset advances the traditional fire detection datasets by considering multiple classes to answer the following question: what is on fire? We perform experiments on four widely used fire detection datasets, and the DFAN provides the best results compared to 21 state-of-the-art methods. Consequently, our research provides a baseline for fire detection over edge devices with higher accuracy and better FPS values, and the proposed dataset extension provides indoor fire classes and a greater number of outdoor fire classes; these contributions can be used in significant future research. Our codes and dataset will be publicly available at https://github.com/tanveer-hussain/DFAN .\"},{\"title\":\"Human activity recognition in artificial intelligence framework: A narrative review\",\"subtitle\":\"Artificial intelligence review\",\"date\":\"August 1, 2022\",\"description\":\"Human activity recognition (HAR) has multifaceted applications due to its worldly usage of acquisition devices such as smartphones, video cameras, and its ability to capture human activity data. While electronic devices and their applications are steadily growing, the advances in Artificial intelligence (AI) have revolutionized the ability to extract deep hidden information for accurate detection and its interpretation. This yields a better understanding of rapidly growing acquisition devices, AI, and applications, the three pillars of HAR under one roof. There are many review articles published on the general characteristics of HAR, a few have compared all the HAR devices at the same time, and few have explored the impact of evolving AI architecture. In our proposed review, a detailed narration on the three pillars of HAR is presented covering the period from 2011 to 2021. Further, the review presents the recommendations for an improved HAR design, its reliability, and stability. Five major findings were: (1) HAR constitutes three major pillars such as devices, AI and applications; (2) HAR has dominated the healthcare industry; (3) Hybrid AI models are in their infancy stage and needs considerable work for providing the stable and reliable design. Further, these trained models need solid prediction, high accuracy, generalization, and finally, meeting the objectives of the applications without bias; (4) little work was observed in abnormality detection during actions; and (5) almost no work has been done in forecasting actions. We conclude that: (a) HAR industry will evolve in terms of the three pillars of electronic devices, applications and the type of AI. (b) AI will provide a powerful impetus to the HAR industry in future.\"},{\"title\":\"NeoAI 1.0: Machine learning-based paradigm for prediction of neonatal and infant risk of death\",\"subtitle\":\"Computers in Biology and Medicine\",\"date\":\"August 1, 2022\",\"description\":\"AbstractBackgroundThe Neonatal mortality rate in the United States is 3.8 deaths per 1000 live births, which is comparably higher than other nations.PurposeThe aim of the proposed study is to design and develop Artificial Intelligence (AI) models (NeoAI 1.0, Global Biomedical Technologies, Inc., Roseville, CA, USA) on risk variables extracted from the National Center for Health Statistics (NCHS) data from 2014 to 2017 duration, consisting of birth-death infant files to predict neonatal and infant deaths.MethodologyThe NCHS data consisted of 15.8 million live birth records, including 91,773 infant deaths, out of which 61,222 were neonatal (life <28 days) and the rest were non-deaths. We designed and developed two different kinds of systems, labelled as neonatal and infant death systems. The data preparation consisted of balancing the two classes using the Adaptive Synthetic oversampling technique (ADASYN) paradigm. The best features were extracted using mutual information followed by 5-fold cross-validation using four different models, namely AdaBoost, XGBoost, Random Forest, and Logistic Regression based on balanced and unbalanced paradigms.ResultsXGBoost gave the best results for the neonatal system with AUC of 0.97 and 0.99 (p < 0.0001), while for the infant system, the scores were 0.91 and 0.99, both systems, without/with ADASYN integration, respectively. Further, there was a 60% increase in F1-score and sensitivity with ADASYN integration. The most important risk factors for classifier models along with feature extraction were maternal age and maternal race by Hispanic classification. Further, gestational age, labour aid and newborn condition were also part of the top five risk factors for these models.ConclusionsNoeAI showed two independent powerful machine learning (ML) systems and selected the best risk predictors combined with classification models for neonatal and infant deaths. The response time of the online platform was less than a second.\"},{\"title\":\"Differential Evolution based compression of CNN for Apple fruit disease classification\",\"subtitle\":\"2022 International Conference on Inventive Computation Technologies (ICICT)\",\"date\":\"July 20, 2022\",\"description\":\"Apple is one of most favourite fruit all over the world. It may get infected due to various diseases such as blotch, scap and rot. This leads to wastage and loss of apple production which incur financial loss to the farmers. It may also have adverse affect on health of people if they consume infected fruits. Thus a deep learning and machine learning based approaches have been investigated to detect the disease at an early stage for its timely treatment. The best accuracy of 96.87% was obtained using deep learning with proposed convolution neural network (CNN) model haing three convolution layers. The CNN models were also compressed using Differential Evolution (DE)-based process and maximum compression of 82.19% was obtained for VGG16 model without any significant loss in performance.\"},{\"title\":\"Eight pruning deep learning models for low storage and high-speed COVID-19 computed tomography lung segmentation and heatmap-based lesion localization: A multicenter study using COVLIAS 2.0\",\"subtitle\":\"Computers in biology and medicine\",\"date\":\"July 1, 2022\",\"description\":\"AbstractBackgroundCOVLIAS 1.0: an automated lung segmentation was designed for COVID-19 diagnosis. It has issues related to storage space and speed. This study shows that COVLIAS 2.0 uses pruned AI (PAI) networks for improving both storage and speed, wiliest high performance on lung segmentation and lesion localization.Methodology: The proposed study uses multicenter ∼9,000 CT slices from two different nations, namely, CroMed from Croatia (80 patients, experimental data), and NovMed from Italy (72 patients, validation data). We hypothesize that by using pruning and evolutionary optimization algorithms, the size of the AI models can be reduced significantly, ensuring optimal performance. Eight different pruning techniques (i) differential evolution (DE), (ii) genetic algorithm (GA), (iii) particle swarm optimization algorithm (PSO), and (iv) whale optimization algorithm (WO) in two deep learning frameworks (i) Fully connected network (FCN) and (ii) SegNet were designed. COVLIAS 2.0 was validated using “Unseen NovMed” and benchmarked against MedSeg. Statistical tests for stability and reliability were also conducted.ResultsPruning algorithms (i) FCN-DE, (ii) FCN-GA, (iii) FCN–PSO, and (iv) FCN-WO showed improvement in storage by 92.4%, 95.3%, 98.7%, and 99.8% respectively when compared against solo FCN, and (v) SegNet-DE, (vi) SegNet-GA, (vii) SegNet-PSO, and (viii) SegNet-WO showed improvement by 97.1%, 97.9%, 98.8%, and 99.2% respectively when compared against solo SegNet. AUC > 0.94 (p < 0.0001) on CroMed and > 0.86 (p < 0.0001) on NovMed data set for all eight EA model. PAI <0.25 s per image. DenseNet-121-based Grad-CAM heatmaps showed validation on glass ground opacity lesions.ConclusionsEight PAI networks that were successfully validated are five times faster, storage efficient, and could be used in clinical settings.\"},{\"title\":\"Cardiovascular Risk Stratification in Diabetic Retinopathy via Atherosclerotic Pathway in COVID-19/Non-COVID-19 Frameworks Using Artificial Intelligence Paradigm: A Narrative Review\",\"subtitle\":\"Diagnositc\",\"date\":\"May 14, 2022\",\"description\":\"Diabetes is one of the main causes of the rising cases of blindness in adults. This microvascular complication of diabetes is termed diabetic retinopathy (DR) and is associated with an expanding risk of cardiovascular events in diabetes patients. DR, in its various forms, is seen to be a powerful indicator of atherosclerosis. Further, the macrovascular complication of diabetes leads to coronary artery disease (CAD). Thus, the timely identification of cardiovascular disease (CVD) complications in DR patients is of utmost importance. Since CAD risk assessment is expensive for low-income countries, it is important to look for surrogate biomarkers for risk stratification of CVD in DR patients. Due to the common genetic makeup between the coronary and carotid arteries, low-cost, high-resolution imaging such as carotid B-mode ultrasound (US) can be used for arterial tissue characterization and risk stratification in DR patients. The advent of artificial intelligence (AI) techniques has facilitated the handling of large cohorts in a big data framework to identify atherosclerotic plaque features in arterial ultrasound. This enables timely CVD risk assessment and risk stratification of patients with DR. Thus, this review focuses on understanding the pathophysiology of DR, retinal and CAD imaging, the role of surrogate markers for CVD, and finally, the CVD risk stratification of DR patients. The review shows a step-by-step cyclic activity of how diabetes and atherosclerotic disease cause DR, leading to the worsening of CVD. We propose a solution to how AI can help in the identification of CVD risk. Lastly, we analyze the role of DR/CVD in the COVID-19 framework.\"},{\"title\":\"RIS-IoT: Towards Resilient, Interoperable, Scalable IoT\",\"subtitle\":\"2022 ACM/IEEE 13th International Conference on Cyber-Physical Systems (ICCPS)\",\"date\":\"May 4, 2022\",\"description\":\"With the introduction of ultra-low-power machine learning (TinyML), IoT devices are becoming smarter as they are driven by ML models. However, any loss of communication at the device level can lead to a failure of the entire IoT system or misleading information trans-mission. Since there exist numerous heterogeneous devices within an IoT system, it is not feasible to centrally monitor all devices or explore system logs to determine communication loss. In this work, to maintain the highest possible communication quality and enable devices adapt according to context changes, we implement a lightweight ML-based adaptive strategy (ASB) and deploy it using a memory-optimized approach over the designed Pycom FiPy based multi-protocol IoT hardware. In real-world ex-periments, ASB equipped FiPy board accurately predicted the RSSI of WiFi 4 & WiFi 5 in real-time and switched between protocols - demonstrating interoperability amongst multiple IoT communication protocols and resilience against communication breakdown.\"},{\"title\":\"Compression and acceleration of convolution neural network: a genetic algorithm based approach\",\"subtitle\":\"Journal of Ambient Intelligence and Humanized Computing\",\"date\":\"March 24, 2022\",\"description\":\"Genetic Algorithm (GA) is a meta-heuristics search and optimization approach which we have utilized for fine-tuning the standard deep learning models for reducing the storage space with improvement in inference time. The pre-trained models have been widely acclaimed as the best choice of CNN for various image classification problems in different domains, but they require huge storage space and it causes a problem for deploying these models on mobile or edge devices. As these devices are constrained with limited memory and computational power. In this paper, a novel GA based method has been proposed which compresses and accelerates the CNN models so that these models can be easily deployed on edge devices. Extensive computer simulations have been conducted on widely used models such as AlexNet, VGG16, SqueezeNet, and ResNet50. We used benchmark datasets such as MNIST, CIFAR-10, and CIFAR-100 to determine the performance. Results reveal that storage space of the AlexNet model was reduced by 87.5%, 86.55% and 86.16% on MNIST, CIFAR-10 and CIFAR- 100 datasets respectively, while VGG16, ResNet50, and SqueezeNet have been compressed by nearly 91%, 78%, 38% respectively. From the results, it has been noticed that there is a significant improvement in inference time of around 35% in AlexNet, 9% in SqueezeNet, 73% in ResNet50, and 80% in VGG16. This improvement is noticed mainly because of the fine-tuning of the deep learning models using the GA. Overall, the proposed GA-based method showed outstanding performance and it motivates research and practitioners to explore it further.\"},{\"title\":\"Cardiovascular disease detection using machine learning and carotid/femoral arterial imaging frameworks in rheumatoid arthritis patients\",\"subtitle\":\"Rheumatology International\",\"date\":\"February 1, 2022\",\"description\":\"The study proposes a novel machine learning (ML) paradigm for cardiovascular disease (CVD) detection in individuals at medium to high cardiovascular risk using data from a Greek cohort of 542 individuals with rheumatoid arthritis, or diabetes mellitus, and/or arterial hypertension, using conventional or office-based, laboratory-based blood biomarkers and carotid/femoral ultrasound image-based phenotypes. Two kinds of data (CVD risk factors and presence of CVD—defined as stroke, or myocardial infarction, or coronary artery syndrome, or peripheral artery disease, or coronary heart disease) as ground truth, were collected at two-time points: (i) at visit 1 and (ii) at visit 2 after 3 years. The CVD risk factors were divided into three clusters (conventional or office-based, laboratory-based blood biomarkers, carotid ultrasound image-based phenotypes) to study their effect on the ML classifiers. Three kinds of ML classifiers (Random Forest, Support Vector Machine, and Linear Discriminant Analysis) were applied in a two-fold cross-validation framework using the data augmented by synthetic minority over-sampling technique (SMOTE) strategy. The performance of the ML classifiers was recorded. In this cohort with overall 46 CVD risk factors (covariates) implemented in an online cardiovascular framework, that requires calculation time less than 1 s per patient, a mean accuracy and area-under-the-curve (AUC) of 98.40% and 0.98 (p < 0.0001) for CVD presence detection at visit 1, and 98.39% and 0.98 (p < 0.0001) at visit 2, respectively. The performance of the cardiovascular framework was significantly better than the classical CVD risk score. The ML paradigm proved to be powerful for CVD prediction in individuals at medium to high cardiovascular risk.\"},{\"title\":\"Designing Framework for Intrusion Detection in IoT Based on Spotted Hyena-Based ANN\",\"subtitle\":\"ICDSMLA 2020: Proceedings of the 2nd International Conference on Data Science, Machine Learning and Applications\",\"date\":\"January 1, 2022\",\"description\":\"Internet of Things (IoT) is the platform for resource sharing amidst the computing platforms, mainly memory, power, large data, and storage. Securing the data in the IoT has become a prime concern as the hackers can access through the invaluable information from the IoT database. Once the attacks are incurred on the IoT environment, the loss of data is inevitable, and it can have a major effect on the progress of the IoT platform. In this paper, an Intrusion Detection System (IDS) with improved artificial intelligence is proposed. This paper makes use of standard benchmark datasets from diverse sources for performing the experiment. A well-performing machine learning algorithm called Artificial Neural Network (ANN) is developed with improved network architecture. The usage of a renowned meta-heuristic algorithm called Spotted Hyena Optimization (SHO) is used for selecting the optimal hidden neurons for ANN. The main objective of the improved training is to reduce the error difference between the target and the measured outputs to enhance the detection accuracy. Finally, the experimental outcomes and simulations prove the stability and robustness of the proposed model in terms of a variety of performance metrics over other machine learning models.\"},{\"title\":\"Object detection system for visually impaired persons using smartphone\",\"subtitle\":\"ICDSMLA 2020: Proceedings of the 2nd International Conference on Data Science, Machine Learning and Applications\",\"date\":\"January 1, 2022\",\"description\":\"Portable assistive technology systems are developed to enhance the capability of persons with disabilities. One of the most important senses for humans is vision. Vision is the most important sense which helps us in understanding the perception of the surrounding environment. Visually impaired persons face a lot of difficulty in understanding the perception around them, particularly in outdoor environment where objects are continuously changing and moving from one place to another. Object detection solutions would greatly assist visually impaired persons in avoiding from the barriers which they face in their daily routine life. The aim of the object detection system is to provide a simple, user-friendly, handy, economical and efficient solution for the visually impaired persons. Motive of this system is to develop a solution that detects the objects present using camera, as the input device in real time and communicate the same to the user using smartphone through headphones. The system would be using an audio device such as speakers or headphones in providing the information about objects to assist the visually impaired persons. The proposed system helps in identifying and avoiding the objects both in outdoor and indoor environments that affect day to day life activities and occupational performance of the visually impaired persons. The information about the objects in surrounding environment would be very much helpful to the visually impaired persons in their daily life.\"},{\"title\":\"A novel compressed and accelerated convolution neural network for covid-19 disease classification: A genetic algorithm based approach\",\"subtitle\":\"Springer\",\"date\":\"December 18, 2021\",\"description\":\"Covid 19 is an infectious disease caused by SARS-Cov-2 virus. It generally affects respiratory system of human and can be fatal if not treated early. It can be caused by coming in contact with an infected person through his/her mouth or nose due to transmission of small liquid particles by way or sneezing or coughing. Since the doctors generally depend on CT scan of suspected patients to confirm if he or she is infected. Proposed research focuses on using CT scan images for Covid-19 diagnosis. In proposed Convolution Neural Network (CNN), there are three convolution layer with 32, 16 and 8 filters in respective layers. The training accuracy of proposed model is 96.71% and testing accuracy is 84.21%. The model was also trained and tested using transfer learning and best test accuracy of 94.73% was obtained using VGG19 pre-trained network. Similarly machine learning methods were also used to classify the images and Random Forest classifier gave best accuracy of 93.33%. Since storage size of pre-trained models was very large hence they were compressed using Genetic Algorithm (GA) without much loss in performance. The VGG16 model could be compressed by 81%, AlexNet by 77.8% and VGG19 by 65.74% without drop in the F1-score. The inference time was also reduced considerably by around 79% for VGG16, 78% for VGG19 and 38% for AlexNet.\"},{\"title\":\"A partcle swarm optimization based approach for filter pruning in convolution neural network for tomato leaf disease classification\",\"subtitle\":\"International Advanced Computing Conference\",\"date\":\"December 18, 2021\",\"description\":\"Since, plant diseases are clearly visible in leaf so, leaves images can be easily used for detection of the disease. Recent research work shows that several machine learning (ML) and deep learning based methods can be used for the classification of images into various classes. Hence in this paper a comparison has been made between machine learning, proposed convolution neural network (CNN) and pre-trained models for the classification of tomato diseases. However, the proposed CNN based model has also been compressed particle swarm optimization based approach so that model can be deployed on devices having less computation power and storage space. For training the model tomato leaf images have been taken from PlantVillage dataset. In PlantVillage dataset, there are 39 classes for various crop but we have used data related to Tomato crop. In tomato crop dataset, there are nine diseased and 1 healthy class. The best accuracy of model using proposed CNN is 98.4% and 94.9% using k-NN tradional ML method. Pre-trained models gives best accuracy of 93.5% using VGG16. The pre-trained CNN models were compressed using Particle Swarm Optimization technique and a compression of around 60% was obtained on VGG16 model size without loss in accuracy. Similarily, the proposed CNN model has also been compressed by 40% with a drop in accuracy of less than 1%.\"},{\"title\":\"Spam detection using ANN and ABC Algorithm\",\"subtitle\":\"2021 11th International Conference on Cloud Computing, Data Science & Engineering (Confluence)\",\"date\":\"November 28, 2021\",\"description\":\"Social network becomes an effective method to engage with our friends. It enables the users to extract a number of in-formation and its usage are increasing day by day. Amidst all the social networking sites, Twitter is one of the interactive social networking services. To change the authorized user accounts, many spammers are utilized a vast amount of spam. Machine Learning (ML) technique is utilized for spam detection system in social sites and for the detection of spammer. Data collection is usually done from H-Spam 14 site with the help of pre-processing mechanism, the data is transforming into lowercase. After the first step, pre-processed data comes under feature extraction phase, which utilizes tokenization process to breakdown each sentence into word group in order to extract the best feature from the raw data. The optimization algorithm referred as Artificial Bee Colony (ABC) is used to pick the optimized value from extracted set of features. It is also utilized to get the optimal sets of features from spam and non-spam data. At the end, performance measure criterion and comparing the existing and proposed work in order to look over the progress of the proposed work. In this work, spam detection system is having higher accuracy, precision, recall, and F-measure as compares to classifiers used previously such as, Naïve Bayes and Support Vector Machine (SVM).\"},{\"title\":\"Ten fast transfer learning models for carotid ultrasound plaque tissue characterization in augmentation framework embedded with heatmaps for stroke risk stratification\",\"subtitle\":\"MDPI\",\"date\":\"November 15, 2021\",\"description\":\"Background and Purpose: Only 1–2% of the internal carotid artery asymptomatic plaques are unstable as a result of >80% stenosis. Thus, unnecessary efforts can be saved if these plaques can be characterized and classified into symptomatic and asymptomatic using non-invasive B-mode ultrasound. Earlier plaque tissue characterization (PTC) methods were machine learning (ML)-based, which used hand-crafted features that yielded lower accuracy and unreliability. The proposed study shows the role of transfer learning (TL)-based deep learning models for PTC. Methods: As pertained weights were used in the supercomputer framework, we hypothesize that transfer learning (TL) provides improved performance compared with deep learning. We applied 11 kinds of artificial intelligence (AI) models, 10 of them were augmented and optimized using TL approaches—a class of Atheromatic™ 2.0 TL (AtheroPoint™, Roseville, CA, USA) that consisted of (i–ii) Visual Geometric Group-16, 19 (VGG16, 19); (iii) Inception V3 (IV3); (iv–v) DenseNet121, 169; (vi) XceptionNet; (vii) ResNet50; (viii) MobileNet; (ix) AlexNet; (x) SqueezeNet; and one DL-based (xi) SuriNet-derived from UNet. We benchmark 11 AI models against our earlier deep convolutional neural network (DCNN) model. Results: The best performing TL was MobileNet, with accuracy and area-under-the-curve (AUC) pairs of 96.10 ± 3% and 0.961 (p < 0.0001), respectively. In DL, DCNN was comparable to SuriNet, with an accuracy of 95.66% and 92.7 ± 5.66%, and an AUC of 0.956 (p < 0.0001) and 0.927 (p < 0.0001), respectively. We validated the performance of the AI architectures with established biomarkers such as greyscale median (GSM), fractal dimension (FD), higher-order spectra (HOS), and visual heatmaps. We benchmarked against previously developed Atheromatic™ 1.0 ML and showed an improvement of 12.9%. Conclusions: TL is a powerful AI tool for PTC into symptomatic and asymptomatic plaques.\"},{\"title\":\"Inter-variability study of COVLIAS 1.0: hybrid deep learning models for COVID-19 lung segmentation in computed tomography\",\"subtitle\":\"Diagnostics\",\"date\":\"November 1, 2021\",\"description\":\"Background: For COVID-19 lung severity, segmentation of lungs on computed tomography (CT) is the first crucial step. Current deep learning (DL)-based Artificial Intelligence (AI) models have a bias in the training stage of segmentation because only one set of ground truth (GT) annotations are evaluated. We propose a robust and stable inter-variability analysis of CT lung segmentation in COVID-19 to avoid the effect of bias. Methodology: The proposed inter-variability study consists of two GT tracers for lung segmentation on chest CT. Three AI models, PSP Net, VGG-SegNet, and ResNet-SegNet, were trained using GT annotations. We hypothesized that if AI models are trained on the GT tracings from multiple experience levels, and if the AI performance on the test data between these AI models is within the 5% range, one can consider such an AI model robust and unbiased. The K5 protocol (training to testing: 80%:20%) was adapted. Ten kinds of metrics were used for performance evaluation. Results: The database consisted of 5000 CT chest images from 72 COVID-19-infected patients. By computing the coefficient of correlations (CC) between the output of the two AI models trained corresponding to the two GT tracers, computing their differences in their CC, and repeating the process for all three AI-models, we show the differences as 0%, 0.51%, and 2.04% (all < 5%), thereby validating the hypothesis. The performance was comparable; however, it had the following order: ResNet-SegNet > PSP Net > VGG-SegNet. Conclusions: The AI models were clinically robust and stable during the inter-variability analysis on the CT lung segmentation on COVID-19 patients.\"},{\"title\":\"Randomly initialized CNN with densely connected stacked autoencoder for efficient fire detection\",\"date\":\"November 1, 2021\",\"description\":\"Vision sensors-based fire detection is an interesting and useful research domain with significant alleviated attention from computer vision experts. The baseline research is based on low-level color features, lately replaced by the effective representation of deep models, achieving better accuracy, but higher false alarm rates still exist with expensive computations. Furthermore, the current feed-forward neural networks initialize and allocate the weights according to the input shape, posing a vanishing gradient problem with slow convergence speed. The main challenges associated with fire detection are the limited performance of the developed models in terms of accuracy, higher false alarm rates, higher computational complexity, and vanishing gradient problems for very deep network architectures. To tackle these issues, herein, we introduce Stacked Encoded-EfficientNet (SE-EFFNet), a cost-aware deep model with reduced false alarm rates and better fire recognition abilities. SE-EFFNet uses a lightweight EfficientNet as a backbone to extract useful features that are further refined using stacked autoencoders before the final classification decision. The stacked autoencoder in SE-EFFNet is not linearly connected, but we use dense connections to ensure effective fire scene recognition, where the weights are randomly initialized to solve the vanishing gradient problems and provide fast convergence speed. The experimental evaluation using benchmarks against recent state-of-the-art demonstrates better recognition abilities of SE-EFFNet and flexible inferencing potentials toward edge devices. We also offer evaluation using favorable lightweight models before selecting the optimal SE-EFFNet.\"},{\"title\":\"Ultrasound-based internal carotid artery plaque characterization using deep learning paradigm on a supercomputer: a cardiovascular disease/stroke risk assessment system\",\"subtitle\":\"The International Journal of Cardiovascular Imaging\",\"date\":\"September 1, 2021\",\"description\":\"Visual or manual characterization and classification of atherosclerotic plaque lesions are tedious, error-prone, and time-consuming. The purpose of this study is to develop and design an automated carotid plaque characterization and classification system into binary classes, namely symptomatic and asymptomatic types via the deep learning (DL) framework implemented on a supercomputer. We hypothesize that on ultrasound images, symptomatic carotid plaques have (a) a low grayscale median because of a histologically large lipid core and relatively little collagen and calcium, and (b) a higher chaotic (heterogeneous) grayscale distribution due to the composition. The methodology consisted of building a DL model of Artificial Intelligence (called Atheromatic 2.0, AtheroPoint, CA, USA) that used a classic convolution neural network consisting of 13 layers and implemented on a supercomputer. The DL model used a cross-validation protocol for estimating the classification accuracy (ACC) and area-under-the-curve (AUC). A sample of 346 carotid ultrasound-based delineated plaques were used (196 symptomatic and 150 asymptomatic, mean age 69.9 ± 7.8 years, with 39% females). This was augmented using geometric transformation yielding 2312 plaques (1191 symptomatic and 1120 asymptomatic plaques). K10 (90% training and 10% testing) cross-validation DL protocol was implemented and showed an (i) accuracy and (ii) AUC without and with augmentation of 86.17%, 0.86 (p-value < 0.0001), and 89.7%, 0.91 (p-value < 0.0001), respectively. The DL characterization system consisted of validation of the two hypotheses: (a) mean feature strength (MFS) and (b) Mandelbrot's fractal dimension (FD) for measuring chaotic behavior. We demonstrated that both MFS and FD were higher in symptomatic plaques compared to asymptomatic plaques by 64.15 ± 0.73% (p-value < 0.0001) and 6 ± 0.13% (p-value < 0.0001), respectively.\"},{\"title\":\"A compressed and accelerated SegNet for plant leaf disease segmentation: a differential evolution based approach\",\"subtitle\":\"Pacific-Asia Conference on knowledge discovery and data mining-Springer\",\"date\":\"August 15, 2021\",\"description\":\"SegNet is a Convolution Neural Network (CNN) architecture consisting of encoder and decoder for pixel-wise classification of input images. It was found to give better results than state of the art pixel-wise segmentation of images. In proposed work, a compressed version of SegNet has been developed using Differential Evolution for segmenting the diseased regions in leaf images. The compressed model has been evaluated on publicly available street scene images and potato late blight leaf images from PlantVillage dataset. Using the proposed method a compression of 25x times is achieved on original SegNet and inference time is reduced by 1.675x times without loss in mean IOU accuracy.\"},{\"title\":\"COVLIAS 1.0: lung segmentation in COVID-19 computed tomography scans using hybrid deep learning artificial intelligence models\",\"subtitle\":\"Diagnostics\",\"date\":\"August 4, 2021\",\"description\":\"Background: COVID-19 lung segmentation using Computed Tomography (CT) scans is important for the diagnosis of lung severity. The process of automated lung segmentation is challenging due to (a) CT radiation dosage and (b) ground-glass opacities caused by COVID-19. The lung segmentation methodologies proposed in 2020 were semi- or automated but not reliable, accurate, and user-friendly. The proposed study presents a COVID Lung Image Analysis System (COVLIAS 1.0, AtheroPoint™, Roseville, CA, USA) consisting of hybrid deep learning (HDL) models for lung segmentation. Methodology: The COVLIAS 1.0 consists of three methods based on solo deep learning (SDL) or hybrid deep learning (HDL). SegNet is proposed in the SDL category while VGG-SegNet and ResNet-SegNet are designed under the HDL paradigm. The three proposed AI approaches were benchmarked against the National Institute of Health (NIH)-based conventional segmentation model using fuzzy-connectedness. A cross-validation protocol with a 40:60 ratio between training and testing was designed, with 10% validation data. The ground truth (GT) was manually traced by a radiologist trained personnel. For performance evaluation, nine different criteria were selected to perform the evaluation of SDL or HDL lung segmentation regions and lungs long axis against GT. Results: Using the database of 5000 chest CT images (from 72 patients), COVLIAS 1.0 yielded AUC of ~0.96, ~0.97, ~0.98, and ~0.96 (p-value < 0.001), respectively within 5% range of GT area, for SegNet, VGG-SegNet, ResNet-SegNet, and NIH. The mean Figure of Merit using four models (left and right lung) was above 94%. On benchmarking against the National Institute of Health (NIH) segmentation method, the proposed model demonstrated a 58% and 44% improvement in ResNet-SegNet, 52% and 36% improvement in VGG-SegNet for lung area, and lung long axis, respectively.\"},{\"title\":\"Multimodality carotid plaque tissue characterization and classification in the artificial intelligence paradigm: a narrative review for stroke application\",\"subtitle\":\"Ann Transl Med.\",\"date\":\"July 1, 2021\",\"description\":\"Cardiovascular disease (CVD) is one of the leading causes of morbidity and mortality in the United States of America and globally. Carotid arterial plaque, a cause and also a marker of such CVD, can be detected by various non-invasive imaging modalities such as magnetic resonance imaging (MRI), computer tomography (CT), and ultrasound (US). Characterization and classification of carotid plaque-type in these imaging modalities, especially into symptomatic and asymptomatic plaque, helps in the planning of carotid endarterectomy or stenting. It can be challenging to characterize plaque components due to (I) partial volume effect in magnetic resonance imaging (MRI) or (II) varying Hausdorff values in plaque regions in CT, and (III) attenuation of echoes reflected by the plaque during US causing acoustic shadowing. Artificial intelligence (AI) methods have become an indispensable part of healthcare and their applications to the non-invasive imaging technologies such as MRI, CT, and the US. In this narrative review, three main types of AI models (machine learning, deep learning, and transfer learning) are analyzed when applied to MRI, CT, and the US. A link between carotid plaque characteristics and the risk of coronary artery disease is presented. With regard to characterization, we review tools and techniques that use AI models to distinguish carotid plaque types based on signal processing and feature strengths. We conclude that AI-based solutions offer an accurate and robust path for tissue characterization and classification for carotid artery plaque imaging in all three imaging modalities. Due to cost, user-friendliness, and clinical effectiveness, AI in the US has dominated the most.\"},{\"title\":\"A new Conv2D model with modified ReLU activation function for identification of disease type and severity in cucumber plant\",\"subtitle\":\"Sustainable Computing: Informatics and Systems\",\"date\":\"June 1, 2021\",\"description\":\"Cucumber is one of the important crop and farmers of most of the counties are cultivating the cucumber crop. Generally, this crop is infected with Angular Spot, Anthracnose, etc. In past research community has developed various learning models to identify the disease in cucumber crop in early-stage and reported maximum accuracy of 85.7%. In proposed work, a convolution neural network based approach has been discussed and disease identification is improved by 8.05% by achieving the accuracy of 93.75%. The proposed model has been trained on a different combination of hyperparameters and activation function. However, the best accuracy has been achieved by introducing a modified ReLU activation function. A segmentation algorithm has also been proposed to estimate the severity of the disease. To establish the efficacy of the proposed model, its performance has been compared with other CNN models as well as traditional machine learning methods.\"},{\"title\":\"Decentralized Accreditation of Educational Attainments using Blockchain\",\"subtitle\":\"IEEE\",\"date\":\"May 1, 2021\",\"description\":\"Today's world is connected through the digital platforms Independent of Geographical location and to serve the needs of the administration process involved in educational we need the platform that help to work through network in a secured and transparent manner and due to the Blockchain core behaviors like transparent, confidentiality, Immutable, decentralized and distributed nature, the Legacy Administrative process for Students Certificate management and verification can be replaced. The Proposed model also helps to setup a streamlined administration process in the educational institute solve the variety of issues that are there in legacy educational organization/institutes administration process like tracking of records in cloud, fraud certification detection, keeping Identity digital under the control of authentic source and transferring over the internet securely, by eliminating need to verify them by third party sources. In this paper we proposed a system to solve the current cumbersome administration process by reducing the huge paperwork involved and by reducing the cost of double spend for manpower among the participating, reduce the wait time for the involved stake holders like Students, guardians and universities.\"},{\"title\":\"INTERVENOR: Intelligent Border Surveillance using Sensors and Drones\",\"date\":\"May 1, 2021\",\"description\":\"Borders play an integral part in a country's security, but Borders are extremely vulnerable and prone to terrorist assaults, illegal smuggling of drugs, and unauthorized immigration. To achieve improved situation analysis, it is essential to fuse sensor-based detection with drone surveillance system to aid the present surveillance system. A wide range of threats can be detected, monitored, and classified by the system, it provides real-time actionable intelligence; endures difficult environments and remote location. Defending borders from unlawful entry of individuals, guns, narcotics and illicit goods is crucial to the protection of a country, and so are ground troops. This special issue aims to give simultaneous erudite examinations of border security by conducting more focus over a specific field of inquiry involved in the securing of borders. We proposed a solution to design an architecture that supports the sharing of information from heterogeneous sources. We are using various sensors which we will install around the perimeter of an area to detect any unknown disturbance (intrusion) around it. With the help of the gathered sensor data, the drone surveillance will be automatically activated and the location of the disturbance will also be sent to the drone and the soldiers in the nearby region, which will help us to track the disturbance so that we can apprehend the treason. We will install camera in the drone which will help us for tracking the target if any.\"},{\"title\":\"A narrative review on characterization of acute respiratory distress syndrome in COVID-19-infected lungs using artificial intelligence\",\"subtitle\":\"Computers in Biology and Medicine\",\"date\":\"March 1, 2021\",\"description\":\"COVID-19 has infected 77.4 million people worldwide and has caused 1.7 million fatalities as of December 21, 2020. The primary cause of death due to COVID-19 is Acute Respiratory Distress Syndrome (ARDS). According to the World Health Organization (WHO), people who are at least 60 years old or have comorbidities that have primarily been targeted are at the highest risk from SARS-CoV-2.Medical imaging provides a non-invasive, touch-free, and relatively safer alternative tool for diagnosis during the current ongoing pandemic. Artificial intelligence (AI) scientists are developing several intelligent computer-aided diagnosis (CAD) tools in multiple imaging modalities, i.e., lung computed tomography (CT), chest X-rays, and lung ultrasounds. These AI tools assist the pulmonary and critical care clinicians through (a) faster detection of the presence of a virus, (b) classifying pneumonia types, and (c) measuring the severity of viral damage in COVID-19-infected patients. Thus, it is of the utmost importance to fully understand the requirements of for a fast and successful, and timely lung scans analysis.This narrative review first presents the pathological layout of the lungs in the COVID-19 scenario, followed by understanding and then explains the comorbid statistical distributions in the ARDS framework. The novelty of this review is the approach to classifying the AI models as per the by school of thought (SoTs), exhibiting based on segregation of techniques and their characteristics. The study also discusses the identification of AI models and its extension from non-ARDS lungs (pre-COVID-19) to ARDS lungs (post-COVID-19). Furthermore, it also presents AI workflow considerations of for medical imaging modalities in the COVID-19 framework.\"},{\"title\":\"A Novel Block Imaging Technique Using Nine Artificial Intelligence Models for COVID-19 Disease Classification, Characterization and Severity Measurement in Lung Computed Tomography Scans on an Italian Cohort\",\"subtitle\":\"Journal of Medical Systems\",\"date\":\"March 1, 2021\",\"description\":\"Computer Tomography (CT) is currently being adapted for visualization of COVID-19 lung damage. Manual classification and characterization of COVID-19 may be biased depending on the expert’s opinion. Artificial Intelligence has recently penetrated COVID-19, especially deep learning paradigms. There are nine kinds of classification systems in this study, namely one deep learning-based CNN, five kinds of transfer learning (TL) systems namely VGG16, DenseNet121, DenseNet169, DenseNet201 and MobileNet, three kinds of machine-learning (ML) systems, namely artificial neural network (ANN), decision tree (DT), and random forest (RF) that have been designed for classification of COVID-19 segmented CT lung against Controls. Three kinds of characterization systems were developed namely (a) Block imaging for COVID-19 severity index (CSI); (b) Bispectrum analysis; and (c) Block Entropy. A cohort of Italian patients with 30 controls (990 slices) and 30 COVID-19 patients (705 slices) was used to test the performance of three types of classifiers. Using K10 protocol (90% training and 10% testing), the best accuracy and AUC was for DCNN and RF pairs were 99.41 ± 5.12%, 0.991 (p < 0.0001), and 99.41 ± 0.62%, 0.988 (p < 0.0001), respectively, followed by other ML and TL classifiers. We show that diagnostics odds ratio (DOR) was higher for DL compared to ML, and both, Bispecturm and Block Entropy shows higher values for COVID-19 patients. CSI shows an association with Ground Glass Opacities (0.9146, p < 0.0001). Our hypothesis holds true that deep learning shows superior performance compared to machine learning models. Block imaging is a powerful novel approach for pinpointing COVID-19 severity and is clinically validated.\"},{\"title\":\"Six artificial intelligence paradigms for tissue characterisation and classification of non-COVID-19 pneumonia against COVID-19 pneumonia in computed tomography lungs\",\"subtitle\":\"International journal of computer assisted radiology and surgery\",\"date\":\"March 1, 2021\",\"description\":\"AbstractBackgroundCOVID-19 pandemic has currently no vaccines. Thus, the only feasible solution for prevention relies on the detection of COVID-19-positive cases through quick and accurate testing. Since artificial intelligence (AI) offers the powerful mechanism to automatically extract the tissue features and characterise the disease, we therefore hypothesise that AI-based strategies can provide quick detection and classification, especially for radiological computed tomography (CT) lung scans.MethodologySix models, two traditional machine learning (ML)-based (k-NN and RF), two transfer learning (TL)-based (VGG19 and InceptionV3), and the last two were our custom-designed deep learning (DL) models (CNN and iCNN), were developed for classification between COVID pneumonia (CoP) and non-COVID pneumonia (NCoP). K10 cross-validation (90% training: 10% testing) protocol on an Italian cohort of 100 CoP and 30 NCoP patients was used for performance evaluation and bispectrum analysis for CT lung characterisation.ResultsUsing K10 protocol, our results showed the accuracy in the order of DL > TL > ML, ranging the six accuracies for k-NN, RF, VGG19, IV3, CNN, iCNN as 74.58 ± 2.44%, 96.84 ± 2.6, 94.84 ± 2.85%, 99.53 ± 0.75%, 99.53 ± 1.05%, and 99.69 ± 0.66%, respectively. The corresponding AUCs were 0.74, 0.94, 0.96, 0.99, 0.99, and 0.99 (p-values < 0.0001), respectively. Our Bispectrum-based characterisation system suggested CoP can be separated against NCoP using AI models. COVID risk severity stratification also showed a high correlation of 0.7270 (p < 0.0001) with clinical scores such as ground-glass opacities (GGO), further validating our AI models.ConclusionsWe prove our hypothesis by demonstrating that all the six AI models successfully classified CoP against NCoP due to the strong presence of contrasting features such as ground-glass opacities (GGO), consolidations, and pleural effusion in CoP patients. Further, our online system takes < 2 s for inference.\"},{\"title\":\"Wilson disease tissue classification and characterization using seven artificial intelligence models embedded with 3D optimization paradigm on a weak training brain magnetic resonance imaging datasets: a supercomputer application\",\"subtitle\":\"Medical & Biological Engineering & Computing\",\"date\":\"March 1, 2021\",\"description\":\"Wilson’s disease (WD) is caused by copper accumulation in the brain and liver, and if not treated early, can lead to severe disability and death. WD has shown white matter hyperintensity (WMH) in the brain magnetic resonance scans (MRI) scans, but the diagnosis is challenging due to (i) subtle intensity changes and (ii) weak training MRI when using artificial intelligence (AI). Design and validate seven types of high-performing AI-based computer-aided design (CADx) systems consisting of 3D optimized classification, and characterization of WD against controls. We propose a “conventional deep convolution neural network” (cDCNN) and an “improved DCNN” (iDCNN) where rectified linear unit (ReLU) activation function was modified ensuring “differentiable at zero.” Three-dimensional optimization was achieved by recording accuracy while changing the CNN layers and augmentation by several folds. WD was characterized using (i) CNN-based feature map strength and (ii) Bispectrum strengths of pixels having higher probabilities of WD. We further computed the (a) area under the curve (AUC), (b) diagnostic odds ratio (DOR), (c) reliability, and (d) stability and (e) benchmarking. Optimal results were achieved using 9 layers of CNN, with 4-fold augmentation. iDCNN yields superior performance compared to cDCNN with accuracy and AUC of 98.28 ± 1.55, 0.99 (p < 0.0001), and 97.19 ± 2.53%, 0.984 (p < 0.0001), respectively. DOR of iDCNN outperformed cDCNN fourfold. iDCNN also outperformed (a) transfer learning–based “Inception V3” paradigm by 11.92% and (b) four types of “conventional machine learning–based systems”: k-NN, decision tree, support vector machine, and random forest by 55.13%, 28.36%, 15.35%, and 14.11%, respectively. The AI-based systems can potentially be useful in the early WD diagnosis.\"},{\"title\":\"Stock Market Prediction using Recurrent Neural Network’s LSTM Architecture\",\"subtitle\":\"2021 IEEE 12th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)\",\"date\":\"February 21, 2021\",\"description\":\"Stock market price prediction is a difficult undertaking that generally requires a lot of human-computer interaction. The stock market process is fraught with risk and is influenced by a variety of factors. Of all the market sectors, it is one of the most volatile and active. When buying and selling stocks from various corporations and businesses, more caution is required. As a result, stock market forecasting is an important endeavor in business and finance. This study analyzes one of the explicit forecasting tactics based on Machine Learning architectures and predictive algorithms and gives an independent model-based strategy for predicting stock prices. The predictor model is based on the Recurrent Neural Networks' LSTM (Long Short-Term Memory) architecture, which specializes in time series data classification and prediction. This model does rigorous mathematical analysis and estimates RMSE to improve forecast accuracy (Root Mean Square Error).All calculations and performance checks are done in Python 3. A number of machine learning libraries are used for prediction and visualization. This study demonstrates that stock performance, sentiment, and social data are all closely related to recent historical data, and it establishes a framework and predicts trading pattern linkages that are suited for High Frequency Stock Trading based on preset parameters using Machine Learning.\"},{\"title\":\"A hybrid deep learning paradigm for carotid plaque tissue characterization and its validation in multicenter cohorts using a supercomputer framework\",\"subtitle\":\"Computers in biology and medicine\",\"date\":\"February 1, 2021\",\"description\":\"AbstractBackgroundEarly and automated detection of carotid plaques prevents strokes, which are the second leading cause of death worldwide according to the World Health Organization. Artificial intelligence (AI) offers automated solutions for plaque tissue characterization. Recently, solo deep learning (SDL) models have been used, but they do not take advantage of the tandem connectivity offered by AI's hybrid nature. Therefore, this study explores the use of hybrid deep learning (HDL) models in a multicenter framework, making this study the first of its kind.MethodsWe hypothesize that HDL techniques perform better than SDL and transfer learning (TL) techniques. We propose two kinds of HDL frameworks: (i) the fusion of two SDLs (Inception with ResNet) or (ii) 10 other kinds of tandem models that fuse SDL with ML. The system Atheromatic™ 2.0HDL (AtheroPoint, CA, USA) was designed on an augmentation framework and three kinds of loss functions (cross-entropy, hinge, and mean-square-error) during training to determine the best optimization paradigm. These 11 combined HDL models were then benchmarked against one SDL model and five types of TL models; thus, this study considers a total of 17 AI models.ResultsAmong the 17 AI models, the best performing HDL system was that comprising CNN and decision tree (DT), as its accuracy and area-under-the-curve were 99.78 ± 1.05% and 0.99 (p<0.0001), respectively. These values are 6.4% and 3.2% better than those recorded for the SDL and TL models, respectively. We validated the performance of the HDL models with diagnostics odds ratio (DOR) and Cohen and Kappa statistics; here, HDL outperformed DL and TL by 23% and 7%, respectively. The online system ran in <2 s.ConclusionHDL is a fast, reliable, and effective tool for characterizing the carotid plaque for early stroke risk stratification.\"},{\"title\":\"Development of web and mobile based smart online healthcare system\",\"subtitle\":\"2021 IEEE 12th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)\",\"date\":\"January 20, 2021\",\"description\":\"Health care system is one of the fundamental parts of society. This paper presents the development of web and smartphone applications for the people of Bangladesh where both patients and doctors can register and patients can have medical treatment via video calling. Using the developed web and smartphone applications, patients can register via their phone number, store information concerning their health, search available doctors, send text messages or make video calls to the available doctors, set alarm to take medicine on time and complete payment through online. Doctors can also register to the system where the verification to ensure the authenticity of the information provided by the doctor is handled manually. The doctors can view a patient’s medical history whenever a patient makes contact through text or video call and provide treatment accordingly. The system has been deployed and verified. The applications provide a faster and enhanced way to get treatment and will ensure the availability of treatments in remote areas. The system will help to spread health care services nationwide and provide doctors with the opportunity to help to improve the health conditions of the citizens of Bangladesh.\"},{\"title\":\"A multicenter study on carotid ultrasound plaque tissue characterization and classification using six deep artificial intelligence models: a stroke application\",\"subtitle\":\"IEEE Transactions on Instrumentation and Measurement\",\"date\":\"January 18, 2021\",\"description\":\"Atherosclerotic plaque in carotid arteries can ultimately lead to cerebrovascular events if not monitored. The objectives of this study are (a) to design a set of artificial intelligence (AI)-based tissue characterization and classification (TCC) systems (Atheromatic 2.0, AtheroPoint, CA, USA) using ultrasound-based carotid artery plaque scans collected from multiple centers and (b) to evaluate the AI performance. We hypothesize that symptomatic plaque is more scattered than asymptomatic plaque. Therefore, the AI system can learn, characterize, and classify them automatically. We developed six kinds of AI systems: four machine learning (ML) systems, one transfer learning (TL) system, and one deep learning (DL) architecture with different layers. Atheromatic 2.0 uses two types of plaque characterization: (a) an AI-based mean feature strength (MFS) and (b) bispectrum analysis. Three kinds of data were collected: London, Lisbon, and Combined (London + Lisbon). We balanced and then augmented five folds to conduct 3-D optimization for optimal number of AI layers versus folds. Using K10 (90% training, 10% testing), the mean accuracies for DL, TL, and ML over the mean of the three data sets were 93.55%, 94.55%, and 89%, respectively. The corresponding mean AUCs were 0.938, 0.946, and 0.889 (p <; 0.0001), respectively. AI paradigms showed an improvement by 10.41% and 3.32% for London and Lisbon in comparison to Atheromatic 1.0, respectively. On characterization, for all three data sets, MFS (symptomatic) > MFS (asymptomatic) by 46.56%, 19.40%, and 53.84%, respectively, thus validating our hypothesis. Atheromatic 2.0 showed consistent and stable results and is useful for carotid plaque tissue classification and characterization for vascular surgery applications.\"},{\"title\":\"EEPMS: energy efficient path planning for mobile sink in wireless sensor networks: a genetic algorithm-based approach\",\"subtitle\":\"Advances in Computational Intelligence and Communication Technology: Proceedings of CICT 2019\",\"date\":\"January 1, 2021\",\"description\":\"The area of wireless sensor networks (WSNs) has been highly explored due to its vast application in various domains. The main constraint of WSN is the energy of the sensor nodes. The use of mobile sink (MS) is one of the prominent methods to preserve the energy of sensor nodes. Moreover, use of mobile sink is also solving the hot-spot problem of wireless sensor network. In the paper, we propose a genetic algorithm-based approach to plan the path for mobile sink. All the basic intermediate operations of genetic algorithms, i.e., chromosome representation, crossover and mutation are well explained with suitable examples. The proposed algorithm is shown its efficacy over the randomly generated path.\"},{\"title\":\"Plant leaf disease segmentation using compressed UNet architecture\",\"subtitle\":\"Trends and Applications in Knowledge Discovery and Data Mining: PAKDD 2021 Workshops, WSPA, MLMEIN, SDPRA, DARAI, and AI4EPT, Delhi, India, May 11, 2021 Proceedings 25\",\"date\":\"January 1, 2021\",\"description\":\"In proposed work, a compressed version of UNet has been developed using Differential Evolution for segmenting the diseased regions in leaf images. The compressed model has been evaluated on potato late blight leaf images from PlantVillage dataset. The compressed model needs only 6.8% of space needed by original UNet architecture, and the inference time for disease classification is twice as fast without loss in performance metric of mean Intersection over Union (IoU).\"},{\"title\":\"Static and dynamic activities prediction of human using machine and deep learning models\",\"subtitle\":\"Innovations in Computer Science and Engineering: Proceedings of 8th ICICSE\",\"date\":\"January 1, 2021\",\"description\":\"Home Innovations in Computer Science and Engineering Conference paperStatic and Dynamic Activities Prediction of Human Using Machine and Deep Learning ModelsS. Valai Ganesh, Mohit Agarwal, Suneet Kr. Gupta & S. Rajakarunakaran Conference paperFirst Online: 24 April 2021444 Accesses1 CitationsPart of the Lecture Notes in Networks and Systems book series (LNNS,volume 171)AbstractRecent advancement in smart phones and computing technologies has played a vital role in people’s life. Develop a model to detect the human basic dynamic activities such as Amble, Climb stairs, coming down the stairs into the floor and human basic static activities like Sitting, Standing or Laying using the person’s smart phone and computers are the major work of this paper. Conventional Machine learning models like Logistic Regression, SVC, Decision tree, etc. results are compared with a recurrent deep neural network model named as Long Short Term Memory (LSTM). LSTM is proposed to detect the human behavior based on Human Activity Recognition (HAR) dataset. The data is monitored and recorded with the aid of sensors like accelerometer and Gyroscope in the user smart phone. HAR dataset is collected from 30 persons, performing different activities with a smart phone to their waists. The testing of the model is evaluated with respect to accuracy and efficiency. The designed activity recognition system can be manipulated in other activities like predicting abnormal human actions, disease by human actions, etc. The overall accuracy has improved to 95.40%.\"},{\"title\":\"Brain MRI‐based Wilson disease tissue classification: An optimised deep transfer learning approach\",\"subtitle\":\"Electronics Letters\",\"date\":\"December 1, 2020\",\"description\":\"Wilson's disease (WD) is caused by the excessive accumulation of copper in the brain and liver, leading to death if not diagnosed early. WD shows its prevalence as white matter hyperintensity (WMH) in MRI scans. It is challenging and tedious to classify WD against controls when comparing visually, primarily due to subtle differences in WMH. This Letter presents a computer-aided design-based automated classification strategy that uses optimised transfer learning (TL) utilising two novel paradigms known as (i) MobileNet and (ii) the Visual Geometric Group-19 (VGG-19). Further, the authors benchmark TL systems against a machine learning (ML) paradigm. Using four-fold augmentation, VGG-19 is superior to MobileNet demonstrating accuracy and area under the curve (AUC) pairs as 95.46 ± 7.70 %, 0.932 (p < 0.0001 ) and 86.87 ± 2.23 %, 0.871 (p < 0.0001 ), respectively. Further, MobileNet and VGG-19 showed an improvement of 3.4 and 13.5%, respectively, when benchmarked against the ML-based soft classifier – Random Forest.\"},{\"title\":\"Development of Efficient CNN model for Tomato crop disease identification\",\"subtitle\":\"Sustainable Computing: Informatics and Systems\",\"date\":\"December 1, 2020\",\"description\":\"Tomato is an important vegetable crop cultivated worldwide coming next only to potato. However, the crop can be damaged due to various diseases. It is important for the farmer to know the type of disease for timely treatment of the crop. It has been observed that leaves are clear indicator of specific diseases. A number of Machine Learning (ML) algorithms and Convolution Neural Network (CNN) models have been proposed in literature for identification of tomato crop diseases. CNN models are based on Deep Learning Neural Networks and differ inherently from traditional Machine Learning algorithms like k-NN, Decision-Trees etc. While pretrained CNN models perform fairly well, they tend to be computationally heavy due to large number of parameters involved. In this paper a simplified CNN model is proposed comprising of 8 hidden layers. Using the publicly available dataset PlantVillage, proposed light weight model performs better than the traditional machine learning approaches as well as pretrained models and achieves an accuracy of 98.4%. PlantVillage dataset comprises of 39 classes of different crops like apple, potato, corn, grapes etc. of which 10 classes are of tomato diseases. While traditional ML methods gives best accuracy of 94.9% with k-NN, best accuracy of 93.5% is obtained with VGG16 in pretrained models. To increase performance of proposed CNN, image pre-processing has been used by changing image brightness by a random value of a random width of image after image augmentation. The proposed model also performs extremely well on dataset other than PlantVillage with accuracy of 98.7%.\"},{\"title\":\"3-D optimized classification and characterization artificial intelligence paradigm for cardiovascular/stroke risk stratification using carotid ultrasound-based delineated plaque: Atheromatic™ 2.0\",\"subtitle\":\"Computers in Biology and Medicine\",\"date\":\"October 1, 2020\",\"description\":\"We hypothesize that symptomatic plaque is hypoechoic due to its large lipid core and minimal collagen, as well as its heterogeneous makeup. Meanwhile, asymptomatic plaque is hyperechoic due to its small lipid core, abundant collagen, and the fact that it is often calcified. We designed a computer-aided diagnosis (CADx) system consisting of three kinds of deep learning (DL) classification paradigms: Deep Convolutional Neural Network (DCNN), Visual Geometric Group-16 (VGG16), and transfer learning, (tCNN). DCNN was 3-D optimized by varying the number of CNN layers and data augmentation frameworks. The DL systems were benchmarked against four types of machine learning (ML) classification systems, and the CADx system was characterized using two novel strategies consisting of DL mean feature strength (MFS) and a bispectrum model using higher-order spectra.\"},{\"title\":\"A robust copy move forgery classification using end to end convolution neural network\",\"subtitle\":\"IEEE\",\"date\":\"June 4, 2020\",\"description\":\"in this digital world it is not surprising to do manipulation with digital images. With advantage of such technologies it has become very easy to misguide the observer about the reality appearing in the images. The objective behind such manipulation for fun and entertainment is acceptable but when such things are applied on sensitive information such as evidences used in judiciary system, to prove certain claim, using such manipulated images on social media becomes dangerous. Although many types of forgeries that could be performed with images such as copying certain part of image then pasting it in same image somewhere else in document with such precision that it appears normal to observer called copy move forgery. Other forgeries include splicing of images, image morphing, retouching etc. Two different categories of approaches are being used till recently for this problem of copy-move forgery detection. These are block based and Keypoint based approach wherein block based is computationally intensive and suffers from many disadvantages and other is based interest points or high textured areas whose features vectors are formed for comparison to find the duplicated regions. In this paper, a deep neural network based approach has been proposed with promising results that can classify images based whether any copy move forgery has been there in the images. The proposed work aims to classify all the images having copy move forgery with presence of scaling, rotation, different compression level. A new CNN model has been researched for this problem to obtain the accuracy of around 93-95 percent for different datasets alone as well on the combination of two or more datasets.\"},{\"title\":\"Convoluted cosmos: classifying galaxy images using deep learning\",\"subtitle\":\"Data Management, Analytics and Innovation: Proceedings of ICDMAI 2019, Volume 1\",\"date\":\"January 1, 2020\",\"description\":\"In this paper, a deep learning-based approach has been developed to classify the images of galaxies into three major categories, namely, elliptical, spiral, and irregular. The classifier successfully classified the images with an accuracy of 97.3958%, which outperformed conventional classifiers like Support Vector Machine and Naive Bayes. The convolutional neural network architecture involves one input convolution layer having 16 filters, followed by 4 hidden layers, 1 penultimate dense layer, and an output Softmax layer. The model was trained on 4614 images for 200 epochs using NVIDIA-DGX-1 Tesla-V100 Supercomputer machine and was subsequently tested on new images to evaluate its robustness and accuracy.\"},{\"title\":\"Potato crop disease classification using convolutional neural network\",\"subtitle\":\"Springer\",\"date\":\"January 1, 2020\",\"description\":\"Potato is one of the most cultivated and in-demand crops after rice and wheat. Potato farming dominates as an occupation in the agriculture domain in more than 125 countries. However, even these crops are, subjected to infections and diseases, mostly categorized into two grades: (i) Early blight and (ii) Late blight. Moreover, these diseases lead to damage the crop and decreases its production. In this paper, we propose a deep learning-based approach to detect the early and late blight diseases in potato by analyzing the visual interpretation of the leaf of several potato crops. The experimental results demonstrate the efficiency of the proposed model even under adverse situations such as variable backgrounds, varying image sizes, spatial differentiation, a high-frequency variation of grades of illumination, and real scene images. In the proposed Convolution Neural Network Architecture (CNN), there are four convolution layers with 32, 16, and 8 filters in each respective layer. The training accuracy of the proposed model is obtained to be 99.47% and testing accuracy is 99.8%.\"},{\"title\":\"sciencedirect.com ToLeD: Tomato leaf disease detection using convolution neural network\",\"subtitle\":\"Procedia Computer Science\",\"date\":\"January 1, 2020\",\"description\":\"Tomato is the most popular crop in the world and in every kitchen, it is found in different forms irrespective of the cuisine. After potato and sweet potato, it is the crop which is cultivated worldwide. India ranked 2 in the production of tomato. However, the quality and quantity of tomato crop goes down due to the various kinds of diseases. So, to detect the disease a deep learning-based approach is discussed in the article. For the disease detection and classification, a Convolution Neural Network based approach is applied. In this model, there are 3 convolution and 3 max pooling layers followed by 2 fully connected layer. The experimental results shows the efficacy of the proposed model over pre-trained model i.e. VGG16, InceptionV3 and MobileNet. The classification accuracy varies from 76% to 100% with respect to classes and average accuracy of the proposed model is 91.2% for the 9 disease and 1 healthy class\"},{\"title\":\"SecureDorm: sensor-based girls hostel surveillance system\",\"subtitle\":\"Smart Systems and IoT: Innovations in Computing: Proceeding of SSIC 2019\",\"date\":\"January 1, 2020\",\"description\":\"The intent is of developing a technology-driven security system for hostels to provide higher security to hostel inmates, which ultimately will help to generate the trust in the parents and guardians so that they will be motivated and encouraged to send their wards to hostels for education. The system is based on Wireless Sensor Network (WSN), which helps to keep the track of system components and their status. And these components are majorly sensors which keep track of breach made. We have proposed a communication system between various sensors to the server for identifying any breaches in the hostel. We have used ultrasonic sensor and GSM module for the implementation. Since the application requires us to install the components on the parameters of the hostel, it should be available.\"},{\"title\":\"A fast and light weight deep convolution neural network model for cancer disease identification in human lung (s)\",\"subtitle\":\"IEEE\",\"date\":\"December 16, 2019\",\"description\":\"In the proposed work, a convolution neural network (CNN) based model has been used to identify the cancer disease in human lung(s). Moreover, this approach identifies the single or multi-module in lungs by analyzing the Computer Tomography (CT) scan. For the purpose of the experiment, publicly available dataset named as Early Lung Cancer Action Program (ELCAP) has been used. Moreover, the performance of proposed CNN model has been compared with traditional machine learning approaches i.e. support vector machine, k-NN, Decision Tree, Random Forest, etc under various parameters i.e. accuracy, precision, recall, Cohen Kappa. The performance of proposed model is also compared with famous CNN models i.e. VGG16, Inception V3 in terms of accuracy, storage space and inference time. The experimental results show the efficacy of proposed algorithms over traditional machine learning and pre-trained models by achieving the accuracy of 99.5%.\"},{\"title\":\"A convolution neural network based approach to detect the disease in corn crop\",\"subtitle\":\"IEEE\",\"date\":\"December 13, 2019\",\"description\":\"The agricultural production is affected by the climate changes i.e. humidity, rain, extremes of temperature etc. Additionally, abiotic stresses are causative element to the etiology of disease as well as pest on crops. The production of the crops can be improved by diagnosis as well as detecting the accurate disease on time or in early stage. Moreover, it is very difficult for accurately detecting and treatment based on the technique which used in disease and insect pests diagnosis. Few researchers have made efforts on predicting disease as well as pest crops using machine learning algorithms. Therefore, this paper presents disease identification in corn crops by analyzing the leaves in the very early stage. We have used PlantVillage dataset for experiments and analysis. The validity of the results has been cheeked on various performance metrics such as precision, accuracy, recall, storage space, running time of the model and AUC-RoC. The obtained results shows the proposed technique outperform in comparison with the traditional machine learning algorithms. Developed model is able to achieve the accuracy of 94%.\"},{\"title\":\"Profit or Loss: A Long Short Term Memory based model for the Prediction of share price of DLF group in India\",\"subtitle\":\"2019 IEEE 9th International Conference on Advanced Computing (IACC), IEEE\",\"date\":\"December 13, 2019\",\"description\":\"Presently, the prediction of share is a challenging issue for the research community as share market is a chaotic place. The reason behind it, there are several factors such as government policies, international market, weather, performance of company. In this article, a model has been developed using long short term memory (LSTM) to predict the share price of DLF group. Moreover, for the experimental purpose the data of DLF group has been taken from yahoo financial services in the time duration of 2008 to 2018 and the recurrent neural network (RNN) model has been trained using data ranging from 2008 to 2017. This RNN based model has been tested on the data of year 2018. For the performance comparison purpose, other linear regression algorithms i.e. k-nn regression, lasso regression, XGboost etc has been executed and the proposed algorithm outperforms with 2.6% root mean square error.\"},{\"title\":\"FCNN-LDA: A Faster Convolution Neural Network model for Leaf Disease identification on Apple's leaf dataset\",\"subtitle\":\"IEEE\",\"date\":\"July 18, 2019\",\"description\":\"Fruits are common items bought by every household. They are delicious to eat and rich in nourishment. However they may also adversely affect health if the fruits are from a diseased tree/plant. Moreover, Farmers may also loose lot of amount of profit if their plants get affected by some disease. In this article, the main objective/goal is to develop a convolution neural network based approach to identify the disease in apple fruit. The data for experiment has been taken from PlantVillage. In the proposed work, a convolution neural network model has been developed to identify the disease in apple and it consists of three convolution layer, three max pooling layer followed by two densely connected layers. This model was formed after testing with varying number of convolution layers from 2 to 6 and found that 3 layer was giving best accuracy. For the result comparison purpose, the traditional machine learning algorithms are also executed on the same dataset. Along with traditional machine learning approaches, the famous pre-trained CNN models i.e. VGG16 and InceptionV3 are also executed. The experiments results shows the efficacy of proposed algorithm over pre-trained models and traditional machine learning approach in terms of accuracy, computational time, specificity, F1 score and AUC-ROC curve. The proposed model achieves the state of the art accuracy of 99%. Moreover, the proposed model requires only 20% of the space as compared to pre-trained model with inference time less than 1 second as pre-trained models require minimum 30 second.\"},{\"title\":\"A Comprehensive Analysis of Image Forensics Techniques: Challenges and future Direction\",\"subtitle\":\"Recent Patents on Engineering\",\"date\":\"July 1, 2019\",\"description\":\"Background: Image forensics deal with the problem of authentication of pictures or their origins. There are two types of forensics techniques namely active and passive. Passive forgery is also known as blind forensics technique. In passive forgery, copy-move (cloning) image forensics is most common forgery technique. In this approach, an object or region of a picture is copied and positioned somewhere else in the same image. The active method used watermarking to solve picture genuineness problem. It has limitations like human involvement or particularly equipped cameras. To overwhelm these limitations, numerous passive authentication approaches have been developed. Moreover, both approaches do not require any prior information about the picture.Objective: The prime objective of this survey is to provide an inclusive summary as well as recent advancement, challenges and future direction in image forensics. In today’s digital era, digital pictures and videos are having a great impact on our life as well as society, as they became an important source of information. Though earlier it was very difficult to doctor the picture, nowadays digital pictures can be doctored easily with the help of editing tools and the internet. These practices make pictures as well as videos genuineness deceptive.Conclusion: This paper presents the current state-of-the-art of passive (cloning) image forensics techniques, challenges and future direction of this research domain. Furthermore, the major open issues in developing a robust cloning image forensics detector with their performance are discussed. Lastly, the available benchmark datasets are also discussed.\"},{\"title\":\"Identification of plant leaf diseases using a nine-layer deep convolutional neural network\",\"subtitle\":\"Computers & Electrical Engineering\",\"date\":\"June 1, 2019\",\"description\":\"In this paper, we proposed a novel plant leaf disease identification model based on a deep convolutional neural network (Deep CNN). The Deep CNN model is trained using an open dataset with 39 different classes of plant leaves and background images. Six types of data augmentation methods were used: image flipping, gamma correction, noise injection, principal component analysis (PCA) colour augmentation, rotation, and scaling. We observed that using data augmentation can increase the performance of the model. The proposed model was trained using different training epochs, batch sizes and dropouts. Compared with popular transfer learning approaches, the proposed model achieves better performance when using the validation data. After an extensive simulation, the proposed model achieves 96.46% classification accuracy. This accuracy of the proposed work is greater than the accuracy of traditional machine learning approaches. The proposed model is also tested with respect to its consistency and reliability.\"},{\"title\":\"Grape Disease Identification Using Convolution Neural Network\",\"subtitle\":\"2019 23rd International Computer Science and Engineering Conference (ICSEC), IEEE\",\"date\":\"January 30, 2019\",\"description\":\"In this paper, we present a CNN model to identify the disease in grapes plant in early stage by analyzing the leaf images. The model is illustrated on publicly available PlantVillage dataset and the performance compared with traditional machine learning models and pre-trained convolution neural network models. The performance of algorithms has been compared on various evaluation metrics i.e. accuracy, precision recall, running time of model, storage space and AUC-RoC. From the experimental results, it has been observed that the performance of proposed model is better than traditional machine learning algorithms as well as pre-trained models by achieving an accuracy of 99%.\"},{\"title\":\"Deep Learning: An Overview and Innovative Approach in Machine Learning\",\"subtitle\":\"IGI Global\",\"date\":\"January 1, 2019\",\"description\":\"Deep learning approaches have been found to be suitable for the agricultural field with successful applications to vegetable infection through plant disease. In this chapter, the authors discuss some widely used deep learning architecture and their practical applications. Nowadays, in many typical applications of machine vision, there is a tendency to replace classical techniques with deep learning algorithms. The benefits are valuable; on one hand, it avoids the need of specialized handcrafted features extractors, and on the other hand, results are not damaged. Moreover, they typically get improved.\"},{\"title\":\"EERP: Energy-Efficient Relay Node Placement for k-Connected Wireless Sensor Networks Using Genetic Algorithm\",\"subtitle\":\"Ambient Communications and Computer Systems: RACCCS-2018\",\"date\":\"January 1, 2019\",\"description\":\"In this article, we propose relay node placement for providing k-connectivity to randomly deployed sensor nodes with energy efficacy using Genetic Algorithm (GA). Here, we also explain the basic step of GA with suitable examples. Also, we carried out the extensive simulations to study proposed algorithm’s performance with existing one in terms of number of deployed nodes and lifetime of the network.\"},{\"title\":\"Wilson's disease: A new perspective review on its genetics, diagnosis and treatment\",\"subtitle\":\"Frontiers in bioscience (Elite edition)\",\"date\":\"January 1, 2019\",\"description\":\"Wilson’s disease (WD) is an autosomal recessive disorder which is caused by poor excretion of copper in mammalian cells. In this review, various issues such as effective characterization of ATP7B genes, scope of gene network topology in genetic analysis, pattern recognition using different computing approaches and fusion possibilities in imaging and genetic dataset are discussed vividly. We categorized this study into three major sections: (A) WD genetics, (B) diagnosis guidelines and (3) treatment possibilities. We addressed the scope of advanced mathematical modelling paradigms for understanding common genetic sequences and dominating WD imaging biomarkers. We have also discussed current state-of-the-art software models for genetic sequencing. Further, we hypothesized that involvement of machine and deep learning techniques in the context of WD genetics and image processing for precise classification of WD. These computing procedures signify changing roles of various data transformation techniques with respect to supervised and unsupervised learning models.\"},{\"title\":\"Pitfree: pot-holes detection on Indian roads using mobile sensors\",\"subtitle\":\"2018 IEEE 8th international advance computing conference (IACC)\",\"date\":\"December 14, 2018\",\"description\":\"Pot-holes on road will make transportation slower and costly. India has a big network of roads to connect the villages and cities, the authority persons cannot travel across the region for identification of holes. As per advancement in machine learning in recent time, we can use this technology for the identification and patching the pot-holes. As per the recent survey around 400 millions, people have a smartphone in India. We can use smartphone sensors (such as Accelerometer and gyroscope) to identify the pot-holes on road and GPS for the location of the pit. The major task of this problem is to capture the data and annotation. We have developed an android app for capturing the value of displacement while travelling on road. We have applied different classification algorithms to sensor raw-data. SVM is the most suitable classification technique for this problem. The android app will sound an alarm when a pothole is detected.\"},{\"title\":\"Energy efficient routing algorithm for wireless sensor networks: A distributed approach\",\"subtitle\":\"Taylor & Francis\",\"date\":\"February 15, 2017\",\"description\":\"In this article, we present an energy efficient routing algorithm for Wireless Sensor Networks (WSNs). Proposed algorithm finds the next hop in each round which is based on the residual energy. The proposed algorithm selects the next hop based on residual energy in each round. The parameters residual energy and distance make the algorithm energy efficient and other parameters ie number of predecessor and successor also improve the chances that each node equally act as the next hop. Experimentally, it is demonstrated that the performance of proposed algorithm is better than existing algorithms in terms of inactive relay nodes, data packet send to BS and consumption of energy in each round.\"},{\"title\":\"GA Based Energy Efficient and Balanced Routing in k-Connected Wireless Sensor Networks\",\"subtitle\":\"Proceedings of the First International Conference on Intelligent Computing and Communication\",\"date\":\"January 1, 2017\",\"description\":\"In the past few years network layer activities in wireless sensor networks gain enormous attention to improve network lifetime. Development of routing algorithms with energy efficacy is one of the most popular techniques to improve it. In this article, energy efficient and balanced route generation algorithm is proposed with considering both energy efficacy and energy balancing issues. Here, we consider the distance and residual energy of the nodes as energy efficiency parameters and energy is balanced by diverting the incoming traffic to other nodes having comparably lower incoming traffic. To develop the routing schedule, we have applied Genetic Algorithm which can quickly compute the routing schedule as per the current state of the network. It is observed that the performance of proposed algorithm is better than existing algorithm in terms of first node die and energy consumption in the network.\"},{\"title\":\"Energy efficient multipath routing for wireless sensor networks: A genetic algorithm approach\",\"subtitle\":\"IEEE\",\"date\":\"September 21, 2016\",\"description\":\"Energy efficiency and fault tolerance are the two most important factors that must be considered for deployment of any Wireless Sensor Network (WSN). Multipath routing is an efficient solution for the fault tolerance of WSNs. In this paper, we propose an algorithm for multipath routing in WSNs which is also energy efficient. The proposed algorithm is based on the popular meta-heuristic technique Genetic Algorithm (GA). In the proposed algorithm, routing schedule is prepared at the base station (BS) which share it with all the nodes of the entire network. The proposed algorithm has an efficient fitness function which is derived with various parameters such as the distance between sender and receiver nodes, the distance between next hop node to the BS and also on the number of hop to send data from next hop node to the BS. The proposed algorithm is tested through simulation and evaluated with various performance metrics.\"},{\"title\":\"Genetic Algorithm for k-Connected Relay Node Placement in Wireless Sensor Networks\",\"subtitle\":\"Proceedings of the Second International Conference on Computer and Communication Technologies: IC3T 2015, Volume 1\",\"date\":\"January 1, 2016\",\"description\":\"Wireless Sensor Networks (WSNs) are widely used for many applications including health care, environment monitoring, underground mines, and so on. In WSN, deployment of relay nodes to cover specific region or target is an important issue. In a target-based WSN, it is important that all the targets must be covered by sensor nodes, and the sensor nodes are connected with the backbone network. In this paper, we propose two algorithms for relay node placement which provide k-connectivity of the sensor nodes. The first algorithm is based on Genetic Algorithm (GA), and the second one is based on greedy approach. We have also to extensively simulate both the algorithms to study their performance.\"},{\"title\":\"Genetic algorithm approach for k-coverage and m-connected node placement in target based wireless sensor networks\",\"date\":\"November 5, 2015\",\"description\":\"In target based wireless sensor networks (WSNs), coverage and connectivity are the two most important issues for guaranteed data forwarding from each target to a remote base station (BS). Given a set of target points, finding minimum number of potential positions to place sensor nodes fulfilling both coverage and connectivity is an NP-complete problem. In this paper, we propose a genetic algorithm (GA) based scheme to solve this problem. Keeping in mind that the sensor nodes are prone to failure, the proposed scheme provides k-coverage to all targets and m-connectivity to each sensor node. Our GA based approach is presented with efficient chromosome representation, derivation of efficient fitness function along with the usual GA operators. The scheme is simulated extensively with various scenarios of WSN. The simulation results are compared with some related existing algorithms to demonstrate the efficacy of the proposed scheme.\"},{\"title\":\"Energy efficient clustering and routing algorithms for wireless sensor networks: GA based approach\",\"subtitle\":\"Wireless Personal Communication\",\"date\":\"August 1, 2015\",\"description\":\"Energy efficient clustering and routing are two well known problems in wireless sensor networks. In this paper, we propose genetic algorithm based approaches for clustering and routing in wireless sensor networks. The clustering is based on residual energy of the gateways and distance from sensor nodes to their corresponding cluster head. The routing scheme is also based on the residual energy of the gateways along with a trade-off between transmission distance and number of forwards. We perform extensive simulations of the proposed algorithms and compare the simulation results with that of the existing algorithms. The results demonstrate that the proposed algorithms outperform the existing algorithms in terms of various performance metrics including energy consumption, number of active nodes, first gateway die and number of dead gateway per round.\"},{\"title\":\"GAE 3BR: Genetic algorithm based energy efficient and energy balanced routing algorithm for Wireless Sensor Networks\",\"date\":\"January 1, 2015\",\"description\":\"The most important task of Wireless Sensor Networks is to gather the information after sensing specified region and pass the same to remotely placed Base Station (BS) directly or by using multi hop communication. In such networks, consumption of energy is one of the most important constraints. In this research article, we discuss an approach to minimize and balance the energy consumption. The proposed algorithm is based on Genetic Algorithm (GA) and it generates such routing scheme which considers both energy balancing and energy efficiency. In proposed algorithm energy consumption issue is considered by minimizing the total distance covered in a round. The Energy balancing issue is taken care by consideration of diverting the incoming traffic of less residual energy relay node to high residual energy relay node. Based on current network state our algorithm quickly computes a new routing schedule. The experimental result shows that the proposed algorithm performs better than the existing techniques.\"},{\"title\":\"E3BFT: Energy efficient and energy balanced fault tolerance clustering in Wireless Sensor Networks\",\"date\":\"November 17, 2014\",\"description\":\"Wireless Sensor Networks (WSNs) consist of sensor nodes, which are small in size with limited energy and computational power. The main function of WSN is to route sensed data to base station (BS), however the main constraint in WSN is irreplaceable power sources of sensor nodes. In this paper, a distributed energy efficient and energy balanced clustering algorithm is proposed. A sensor node selects a cluster head (CH) having minimum routing overhead with higher ratio of residual energy of CH and corresponding distance. Routing overhead of CH is calculated only when the route from individual CH to base station (BS) is defined, so for defining the route amongst CH, we execute GA based routing algorithm named as GAR [1] before selection of CH by sensor nodes. Proposed algorithm also takes care of those sensor nodes, which are not in communication range of CH. This is also complimented with fault tolerant issue of WSNs as the sensor nodes are prone to failure.\"},{\"title\":\"A novel evolutionary approach for load balanced clustering problem for wireless sensor networks\",\"subtitle\":\"Swarm and Evolutionary Computing/Elsevier\",\"date\":\"October 1, 2013\",\"description\":\"Clustering sensor nodes is an effective topology control method to reduce energy consumption of the sensor nodes for maximizing lifetime of Wireless Sensor Networks (WSNs). However, in a cluster based WSN, the leaders (cluster heads) bear some extra load for various activities such as data collection, data aggregation and communication of the aggregated data to the base station. Therefore, balancing the load of the cluster heads is a challenging issue for the long run operation of the WSNs. Load balanced clustering is known to be an NP-hard problem for a WSN with unequal load of the sensor nodes. Genetic Algorithm (GA) is one of the most popular evolutionary approach that can be applied for finding the fast and efficient solution of such problem. In this paper, we propose a novel GA based load balanced clustering algorithm for WSN. The proposed algorithm is shown to perform well for both equal as well as unequal load of the sensor nodes. We perform extensive simulation of the proposed method and compare the results with some evolutionary based approaches and other related clustering algorithms. The results demonstrate that the proposed algorithm performs better than all such algorithms in terms of various performance metrics such as load balancing, execution time, energy consumption, number of active sensor nodes, number of active cluster heads and the rate of convergence.\"},{\"title\":\"Delay Constraint Energy Efficient Routing Using Multi-objective Genetic Algorithm in Wireless Sensor Networks\",\"date\":\"October 1, 2013\"},{\"title\":\"GAR: an energy efficient GA-based routing for wireless sensor networks\",\"subtitle\":\"LNCS/ Springer\",\"date\":\"February 1, 2013\",\"description\":\"Routing with energy consideration has paid enormous attention in the field of Wireless Sensor Networks (WSNs). In Some WSNs, some high energy sensors called relay nodes are responsible to route the data towards a base station. Reducing energy consumption of these relay nodes allow us to prolong the lifetime and coverage of the WSN. In this paper, we present a Genetic algorithm based routing scheme called GAR (Genetic Algorithm-based Routing) that considers the energy consumption issues by minimizing the total distance travelled by the data in every round. Our GA based approach can quickly compute a new routing schedule based on the current network state. The scheme uses the advantage of computational efficiency of GA to quickly find out a solution to the problem. The experimental results demonstrate that the proposed algorithm is better than the existing techniques in terms of network life time, energy consumption and the total distance covered in each round.\"},{\"title\":\"Literature Survey on Design and Implementation of Processing Model for Polarity Identification on Textual Data of English language\",\"subtitle\":\"IJCSI\",\"date\":\"January 1, 2013\"},{\"title\":\"Cross lingual information retrieval with SMT and query mining\",\"subtitle\":\"Advanced Computing\",\"date\":\"September 1, 2011\",\"description\":\"In this paper, we have taken the English Corpus and Queries, both translated and transliterated form. We use Statistical Machine Translator to find the result under translated and transliterated queries and then analyzed the result. These queries wise results can then be undergone mining and therefore a new list of queries is created. We have design an experimental setup followed by various steps which calculate Mean Average Precision. We have taken assistance ship of Terrier Open Source for the Information Retrieval. On the basis of created new query list, we calculate the Mean Average Precision and find a significant result i.e. 93.24% which is very close to monolingual results calculated for English language.\"},{\"title\":\"Systematic review of artificial intelligence in acute respiratory distress syndrome for COVID-19 lung patients: a biomedical imaging perspective\",\"subtitle\":\"IEEE journal of biomedical and health informatics\",\"description\":\"SARS-CoV-2 has infected over ∼165 million people worldwide causing Acute Respiratory Distress Syndrome (ARDS) and has killed ∼3.4 million people. Artificial Intelligence (AI) has shown to benefit in the biomedical image such as X-ray/Computed Tomography in diagnosis of ARDS, but there are limited AI-based systematic reviews (aiSR). The purpose of this study is to understand the Risk-of-Bias (RoB) in a non-randomized AI trial for handling ARDS using novel AtheroPoint-AI-Bias (AP(ai)Bias). Our hypothesis for acceptance of a study to be in low RoB must have a mean score of 80% in a study. Using the PRISMA model, 42 best AI studies were analyzed to understand the RoB. Using the AP(ai)Bias paradigm, the top 19 studies were then chosen using the raw-cutoff of 1.9. This was obtained using the intersection of the cumulative plot of “mean score vs. study” and score distribution. Finally, these studies were benchmarked against ROBINS-I and PROBAST paradigm. Our observation showed that AP(ai)Bias, ROBINS-I, and PROBAST had only 32%, 16%, and 26% studies, respectively in low-moderate RoB (cutoff>2.5), however none of them met the RoB hypothesis. Further, the aiSR analysis recommends six primary and six secondary recommendations for the non-randomized AI for ARDS. The primary recommendations for improvement in AI-based ARDS design inclusive of (i) comorbidity, (ii) inter-and intra-observer variability studies, (iii) large data size, (iv) clinical validation, (v) granularity of COVID-19 risk, and (vi) cross-modality scientific validation. The AI is an important component for diagnosis of ARDS and the recommendations must be followed to lower the RoB.\"}]",
    "patents": "[{\"title\":\"Automatic Compression System\",\"patents_id\":\"202011057506\"},{\"title\":\"Compression And Acceleration of Fully Convolutional Network (FCN) For Image Segmentation To Use In Surveillance and Agriculture Applications: An Approach Based on Particle Swarm Optimization\",\"patents_id\":\"202111026172\"},{\"title\":\"Flawless distributed transmission in multidimensional antenna router\",\"patents_id\":\"359731-001\"},{\"title\":\"IoT Device Classifier using ML\",\"patents_id\":\"202111055728\"},{\"title\":\"Sale Predict: Predict accurate sales for Walmart stores considering the impact of promotional markdown events using Deep Learning Programming\",\"patents_id\":\"202111037617\"},{\"title\":\"Smart Surveillance System\",\"patents_id\":\"202111007706\"}]",
    "projects": "[{\"title\":\"IoT Network Traffic Classification and attack detection based on Network Traffic Characteristics using Artificial Intelligence\",\"start_date\":\"Nov 2020\",\"end_date\":\"May 2021\",\"description\":\"i)Develop a coherent structure for IoT traffic classification. We will extract various features from the IoT network traffic and describe the importance of each feature for the classification.ii) Use a variety of machine learning algorithms and the obtained classification results of each classifier will compare on the basis of accuracy, training time of algorithm, error rate etc.iii) Develop an artificial model for identify the attack pattern by tracing IoT devices traffic.\"},{\"title\":\"Suknya Rakshak: Wireless Sensor Networks based Security System for girls’ hostels\",\"start_date\":\"Jan 2018\",\"end_date\":\"Jan 2020\",\"description\":\"The Project is funded by Council of Science and Technology, Uttar Pradesh Lucknow. The objective of proposed work are as follows:i)Time-based tracking of the movement of students or inmates with the help of WSN based virtual fence and GPRS based communication. ii)Designing of an energy efficient and fault-finding routing algorithm to improve the operational lifetime of WSN and fool-proof security.iii)System design with scalability (can be made to work with all the hostels) and central monitoring mechanism to minimize the deployment and manpower costiv)Design a system which cannot be compromised/manipulated, is scalable (can be made to work with all the hostels) and provides a central monitoring mechanism to minimize the deployment and manpower cost.v)Design of an automated system which identifies any kind of unauthorized movement in the hostel.vi)Deployment of the system as a pilot project for a girls’ hostel and test the working.\"},{\"title\":\"Design and Implementation of speech analysis system to identify the dominating emotion of a candidate: A Deep Learning Approach \",\"start_date\":\"Jan 2019\",\"end_date\":\"May 2019\",\"description\":\"This project was also developed for the Empass Hire. In this project our objective was to identify the dominating emotion of a candidate by analysing the speech. In this project, a deep neural network-based classification model has been trained on audio data. After training, an audio sample is passed in this model and it returns the frequency counts of different emotions thorough out the audio sample.\"},{\"title\":\"Title: Design and Implementation of trait identification module: A Deep Learning Approach on video data\",\"start_date\":\"Jan 2019\",\"end_date\":\"May 2019\",\"description\":\"This project was developed by our team for Empass Hire (https://empass.mobi). Empass Hire is a remote hiring screening tool on mobile which enables early detection of the ideal applicant for companies without an in-person interview. In this project, the objective was to identify the dominating emotion of a candidate by analysing the video captured during the interview. To complete this project, the dataset has been provided by the Empass Hire as well as we also collected the dataset. After collection of the dataset, a deep learning model has been developed which analyse the video and provide the emotions of candidate in regular interval and store the relevant values. After then a graph is returned by the model as an out put which represents that which is the dominating emotion i.e. angry, happy, sad, neutral etc.\"}]",
    "organizations": null,
    "location": "Noida",
    "linkedin_id": "dr-suneet-k-gupta-b2458153",
    "activity": [
        {
            "interaction": "Liked by Dr. Suneet K. Gupta",
            "link": "",
            "title": "Thanks, IEEE Transactions on Network Science and Engineering for acknowledging efforts for the growth of science.",
            "img": "",
            "id": "7278682584694362113"
        },
        {
            "interaction": "Liked by Dr. Suneet K. Gupta",
            "link": "",
            "title": "Understanding Reactions to Thank You Messages I have been reflecting on the responses to my “thank you” messages and how reactions can vary. In a…",
            "img": "",
            "id": "7278342421392900097"
        },
        {
            "interaction": "Liked by Dr. Suneet K. Gupta",
            "link": "",
            "title": "Application Invited (Honorary/Voluntary) Position: Deputy Director Research (Directorate of Mathematical Sciences) Who can apply:…",
            "img": "",
            "id": "7278073413804437504"
        },
        {
            "interaction": "Liked by Dr. Suneet K. Gupta",
            "link": "",
            "title": "ONE WEEK ENTREPRENEURSHIP AND SKILL DEVELOPMENT PROGRAMME (ESDP) ON \"EMPOWERING ENTREPRENEURS: CYBER AND NETWORK SECURITY AWARENESS FOR THREAT…",
            "img": "",
            "id": "7278352542764351488"
        },
        {
            "interaction": "Liked by Dr. Suneet K. Gupta",
            "link": "",
            "title": "Understanding Generation Z I have been observing people who belong to Generation Z and have read extensively about them. Generation Z encompasses…",
            "img": "",
            "id": "7277905751266779136"
        },
        {
            "interaction": "Liked by Dr. Suneet K. Gupta",
            "link": "",
            "title": "🚀 Join us at ESCI-2025! Call for Papers Now Open! 🌟 We are excited to invite researchers and practitioners to contribute to the Advances and Trends…",
            "img": "",
            "id": "7277373278543888384"
        },
        {
            "interaction": "Liked by Dr. Suneet K. Gupta",
            "link": "",
            "title": "I am pleased to share with you that under the CSR partnership with Infosys, Symbiosis International University has established a Infosys springboard…",
            "img": "",
            "id": "7278305224581107712"
        },
        {
            "interaction": "Liked by Dr. Suneet K. Gupta",
            "link": "",
            "title": "Today Prof.(Dr.) Jayanthi Ranjan Hon’ble Vice Chancellor, Sharda University Agra with Shri Awanish Kumar Awasthi Ji (IAS), Chief Advisor, Chief…",
            "img": "",
            "id": "7278023755010826240"
        },
        {
            "interaction": "Liked by Dr. Suneet K. Gupta",
            "link": "",
            "title": "Our new work is accepted by IEEE TDSC. Here is the link: ",
            "img": "",
            "id": "7278045575332159489"
        },
        {
            "interaction": "Liked by Dr. Suneet K. Gupta",
            "link": "",
            "title": "📢 We’re Hiring! Rayat Bahra University is looking for a dynamic Pro Vice Chancellor/Dean Computer Science Engineering (CSE) to join our team!…",
            "img": "",
            "id": "7275423253320364033"
        },
        {
            "interaction": "Liked by Dr. Suneet K. Gupta",
            "link": "",
            "title": "Happy to be part of a big conference themed on Vikashit Bharat! The International Conference on \"Towards Building Positively Responding Society…",
            "img": "",
            "id": "7277932360795365376"
        },
        {
            "interaction": "Liked by Dr. Suneet K. Gupta",
            "link": "",
            "title": "New Vice Chancellor for D Y Patil International University(DYPIU), Akurdi, Pune - After I posted the upcoming advertisement for Vice Chancellor…",
            "img": "",
            "id": "7277327589659058176"
        },
        {
            "interaction": "Liked by Dr. Suneet K. Gupta",
            "link": "",
            "title": "Amazing Surprise for me … Cannot be more proud and happy that my Contribution is well awarded… What a Wow Start of Year 2025…. Really happy to get…",
            "img": "",
            "id": "7277667486991310848"
        },
        {
            "interaction": "Liked by Dr. Suneet K. Gupta",
            "link": "",
            "title": "A patent has been granted to a team of inventors comprising myself and my the then brilliant Computer Science Engineering colleagues, Dr Suneet Kumar…",
            "img": "",
            "id": "7277272817161682944"
        },
        {
            "interaction": "Liked by Dr. Suneet K. Gupta",
            "link": "",
            "title": "I am happy to announce that as Vice President and Assistant Director of Institution’s Innovation Council (IIC) under the Ministry of Education…",
            "img": "",
            "id": "7277163955700092928"
        }
    ],
    "linkedin_num_id": 189530380.0,
    "honors_and_awards": null,
    "input": {
        "url": ""
    }
}